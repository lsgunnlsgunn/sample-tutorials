{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to DC/OS Tutorials Demonstrates the first steps and key tasks for running DC/OS The tutorials in this section cover the following tasks: How to create a small cluster for demonstration and testing purposes. How to define and deploy sample services on the new cluster. How to customize your cluster configuration. These tutorials guide you through the steps for setting up a cluster for testing or evaluation purposes. For a production environment or larger scale deployment, however, additional steps are typically required.","title":"Welcome to DC/OS Tutorials"},{"location":"#welcome-to-dcos-tutorials","text":"","title":"Welcome to DC/OS Tutorials"},{"location":"intro/","text":"Introducing DC/OS Provides step-by-step instructions to introduce basic DC/OS cluster tasks If you are setting up a demonstration environment as a proof-of-concept for testing or evaluation purposes or creating a small cluster for application development or a specific project, you can use the steps in this tutorial to get started using DC/OS. This introduction to DC/OS tutorial covers the following basic tasks: how to download and install cluster files how to create a small cluster how to install and deploy packages, services, and applications how to view and verifying cluster activity For a production environment or larger scale deployment, additional planning and deployment steps are required. For example, you might have sufficient permissions to set up a demonstration environment or small cluster. In a production or enterprise deployment, however, you might have more restrictive privileges or be explicitly assigned specific permissions based on your role. Intended audience This tutorial provides users and application developers an overview of DC/OS basics, including the steps required to deploy and orchestrate a few sample applications in a distributed cluster environment. After completing this tutorial, you will have hands-on experience deploying multiple applications on a cluster. Along the way, the tutorial also suggests links to additional documentation for background or more detailed information into related topics for you to explore further. What you'll find This introduction to DC/OS contains tutorials for working with DC/OS Open Source and Enterprise clusters. Each tutorial illustrates how to accomplish a specific goal. In most cases, each goal involves a sequence of steps. In a production environment, you might need to perform additional steps or choose to perform these steps in a different order. Where to go for additional information Before you get started, you might want to bookmark the Glossary for reference. For example, you can use the Glossary to look up unfamiliar concepts or terms. Ready to get started? Let's create your first cluster ...","title":"Introduction"},{"location":"intro/#introducing-dcos","text":"","title":"Introducing DC/OS"},{"location":"intro/#intended-audience","text":"This tutorial provides users and application developers an overview of DC/OS basics, including the steps required to deploy and orchestrate a few sample applications in a distributed cluster environment. After completing this tutorial, you will have hands-on experience deploying multiple applications on a cluster. Along the way, the tutorial also suggests links to additional documentation for background or more detailed information into related topics for you to explore further.","title":"Intended audience"},{"location":"intro/#what-youll-find","text":"This introduction to DC/OS contains tutorials for working with DC/OS Open Source and Enterprise clusters. Each tutorial illustrates how to accomplish a specific goal. In most cases, each goal involves a sequence of steps. In a production environment, you might need to perform additional steps or choose to perform these steps in a different order.","title":"What you'll find"},{"location":"intro/#where-to-go-for-additional-information","text":"Before you get started, you might want to bookmark the Glossary for reference. For example, you can use the Glossary to look up unfamiliar concepts or terms. Ready to get started? Let's create your first cluster ...","title":"Where to go for additional information"},{"location":"cli/","text":"Install the command-line interface Install the command-line interface to perform your day-to-day tasks The DC/OS command-line interface (CLI) provides a convenient way for you to perform your administrative tasks, retrieve information about components and operations, and monitor cluster status and activity. Although you can perform many of the same tasks interactively using the DC/OS web-based console or programmatically using calls to the DC/OS application programming interface (API), most cluster operators use command-line programs interactively or in scripts to manage most of their common cluster operations and cluster-related activity. Before you begin Before starting this tutorial, you should verify the following: You must be able to access a properly-configured DC/OS cluster with at least at least one master node and three agent nodes from the computer hosting the CLI. You must have an account with administrative privileges for the local operating system. You must be able to open a command-line shell on the computer hosting the CLI. You must be able to run the client URL ( cURL ) program on the computer hosting the CLI. You must disable any security or antivirus software before you start the installation (Windows only). Learning objectives By completing this tutorial, you will learn: - How to download the DC/OS command-line interface (CLI) from the DC/OS web-based administrative console. How to install DC/OS command-line interface (CLI) directly from the package repository How to connect to a cluster from a terminal shell on your local computer using the DC/OS CLI. How to perform common administrative tasks using CLI commands. Install the DC/OS CLI Open a terminal shell on the computer where you want to install the DC/OS command-line interface (CLI). Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click the cluster name menu located in the top-right corner of the DC/OS web-based console. From the cluster name menu , select Install CLI . Click the appropriate operating system tab for the computer where you want to install the CLI. Follow the instructions displayed on the tab for your operating system. For example, for Linux or macOS, copy the code snippet displayed in the Install CLI dialog box and paste it into the terminal shell to download the CLI package to the local computer. Type the password for an administrative account on the local host computer. Type the user name and password for the cluster administrative account. The default administrative user name for the cluster is bootstrapuser . The default password for the account is deleteme . Verify you can connect to the cluster from the command line by running the following command: bash dcos cluster list If the cluster is available and the CLI installation completed successfully, the command returns basic information about the cluster similar to the following: bash NAME ID STATUS VERSION URL * lgunn-sidebet 351c8aa0-880e-459a-9483-cd6a4ab4391e AVAILABLE 1.13.0 http://lgunn-sid-elasticl-1lqsarfasaw88-301095172.us-west-2.elb.amazonaws.com 1. Close the Install CLI dialog box in the DC/OS web-based administrative console. Verify login credentials You can verify an account is authorized to connect to the cluster by running the following command: dcos auth login Initially, only the default cluster administrative account is authorized to connect to the cluster. As you authorize other users to access the cluster and replace the default cluster administrative account and password, you can verify their access to the cluster by running the dcos auth login command. Verify services running on the cluster After you install the DC/OS CLI, there are many commands available for you to check the status of the cluster and perform routine administrative tasks. For example, you can check the list of running services byt running the following command: dcos service If you have just completed the previous tutorial and have not yet installed any additional services, this command returns information similar to the following: NAME HOST ACTIVE TASKS CPU MEM DISK ID marathon 10.0.4.82 True 0 0.0 0.0 0.0 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-0001 metronome 10.0.4.82 True 0 0.0 0.0 0.0 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-0000 These services are installed and started as part of the initial DC/OS installation: - The marathon service is a fundamental component of the DC/OS cluster and provides initialization services for the DC/OS core. - The metronome service provides basic scheduling and job management similar to the cron program for the DC/OS cluster. As you deploy additional services, you can use the dcos service command to verify the status of those services. Check the status of connected nodes You can use the DC/OS command-line interface to check the status of connected nodes and to return log information. As a starting point for exploring your DC/OS cluster, you might want to run the dcos node list command. dcos node list This command returns basic information about the connected agent and master nodes in your cluster. For example: HOSTNAME IP PUBLIC IP(S) ID TYPE REGION ZONE 10.0.2.246 10.0.2.246 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S1 agent aws/us-west-2 aws/us-west-2a 10.0.5.193 10.0.5.193 52.24.8.165 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S0 agent aws/us-west-2 aws/us-west-2a master.mesos. 10.0.4.82 34.220.80.239 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef master (leader) aws/us-west-2 aws/us-west-2a ``` You might also want to explore the logs for individual nodes. For example, you can retrieve detailed information about the current master node leader by running the command `dcos node log --leader`. This command returns the information recorded in log messages for the leading master. The messages logged provide information to this sample entry: ```bash 2019-05-27 18:47:51 UTCbouncer.sh [2847]: 10.0.4.82 [27/May/2019:18:47:51 +0000] GET /acs/api/v1/internal/policyquery?rid=dcos:adminrouter:ops:mesos uid=dcos_history_service action=full HTTP/1.0 200 22 - Master Admin Router (0.001926 s) Similarly, you can retrieve detailed information about a specific node by running a command similar to the following: dcos node log --mesos-id ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S1 In this example, ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S1 is the agent identifier (ID). You can use the dcos node list command to look up the agent identifier, then retrieve log information for the agent. You can then use the information returned to trace agent activity, analyze operation, or troubleshoot potential issues. Getting usage information for CLI programs To explore the types of information available through the DC/OS CLI options, enter the dcos help command. This help option summarizes the top-level of commands available. For example: Usage: dcos [command] Commands: auth Authenticate to DC/OS cluster backup Access DC/OS backup functionality cluster Manage your DC/OS clusters config Manage the DC/OS configuration file help Help about any command job Deploy and manage jobs in DC/OS license Manage your DC/OS licenses marathon Deploy and manage applications to DC/OS node View DC/OS node information package Install and manage DC/OS software packages plugin Manage CLI plugins security DC/OS security related commands service Manage DC/OS services task Manage DC/OS tasks Options: --version Print version information -v, -vv Output verbosity (verbose or very verbose) -h, --help Show usage help You can then use the --help option for individual commands to view usage information about a specific command. For example, you can run dcos node --help to see information about specific dcos node commands and arguments. For more information about working with the DC/OS command-line interface, see the CLI documentation . Next steps Congratulations! You have successfully connected to your cluster using the DC/OS CLI, and started exploring some of the tasks and information available using the DC/OS command-line interface. The next tutorials explore additional getting started tasks that you can perform using the DC/OS web-based administrative console or command-line interface: Install your first service from the package repository Deploy your first sample application Discover deployed services Related topics You have already worked with several core components of the DC/OS architecture, including the Mesos kernel, Marathon, and Metronome. For more information about these DC/OS components and where they fit into the DC/OS platform or features and services, see the following topics in the main DC/OS documentation : Marathon starts and monitors DC/OS applications and services. Apache Mesos is the kernel of DC/OS and responsible for low-level task maintenance. Mesos DNS provides service discovery within the cluster. Minuteman is the internal layer 4 load balancer. Admin router is an open source NGINX configuration that provides central authentication and proxy to DC/OS services. Package repository is the package repository that holds the DC/OS services (e.g. Apache Spark or Apache Cassandra) that you can install on your cluster directly from the DC/OS GUI and CLI.","title":"Install the CLI"},{"location":"cli/#install-the-command-line-interface","text":"","title":"Install the command-line interface"},{"location":"cli/#before-you-begin","text":"Before starting this tutorial, you should verify the following: You must be able to access a properly-configured DC/OS cluster with at least at least one master node and three agent nodes from the computer hosting the CLI. You must have an account with administrative privileges for the local operating system. You must be able to open a command-line shell on the computer hosting the CLI. You must be able to run the client URL ( cURL ) program on the computer hosting the CLI. You must disable any security or antivirus software before you start the installation (Windows only).","title":"Before you begin"},{"location":"cli/#learning-objectives","text":"By completing this tutorial, you will learn: - How to download the DC/OS command-line interface (CLI) from the DC/OS web-based administrative console. How to install DC/OS command-line interface (CLI) directly from the package repository How to connect to a cluster from a terminal shell on your local computer using the DC/OS CLI. How to perform common administrative tasks using CLI commands.","title":"Learning objectives"},{"location":"cli/#install-the-dcos-cli","text":"Open a terminal shell on the computer where you want to install the DC/OS command-line interface (CLI). Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click the cluster name menu located in the top-right corner of the DC/OS web-based console. From the cluster name menu , select Install CLI . Click the appropriate operating system tab for the computer where you want to install the CLI. Follow the instructions displayed on the tab for your operating system. For example, for Linux or macOS, copy the code snippet displayed in the Install CLI dialog box and paste it into the terminal shell to download the CLI package to the local computer. Type the password for an administrative account on the local host computer. Type the user name and password for the cluster administrative account. The default administrative user name for the cluster is bootstrapuser . The default password for the account is deleteme . Verify you can connect to the cluster from the command line by running the following command: bash dcos cluster list If the cluster is available and the CLI installation completed successfully, the command returns basic information about the cluster similar to the following: bash NAME ID STATUS VERSION URL * lgunn-sidebet 351c8aa0-880e-459a-9483-cd6a4ab4391e AVAILABLE 1.13.0 http://lgunn-sid-elasticl-1lqsarfasaw88-301095172.us-west-2.elb.amazonaws.com 1. Close the Install CLI dialog box in the DC/OS web-based administrative console.","title":"Install the DC/OS CLI"},{"location":"cli/#verify-login-credentials","text":"You can verify an account is authorized to connect to the cluster by running the following command: dcos auth login Initially, only the default cluster administrative account is authorized to connect to the cluster. As you authorize other users to access the cluster and replace the default cluster administrative account and password, you can verify their access to the cluster by running the dcos auth login command.","title":"Verify login credentials"},{"location":"cli/#verify-services-running-on-the-cluster","text":"After you install the DC/OS CLI, there are many commands available for you to check the status of the cluster and perform routine administrative tasks. For example, you can check the list of running services byt running the following command: dcos service If you have just completed the previous tutorial and have not yet installed any additional services, this command returns information similar to the following: NAME HOST ACTIVE TASKS CPU MEM DISK ID marathon 10.0.4.82 True 0 0.0 0.0 0.0 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-0001 metronome 10.0.4.82 True 0 0.0 0.0 0.0 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-0000 These services are installed and started as part of the initial DC/OS installation: - The marathon service is a fundamental component of the DC/OS cluster and provides initialization services for the DC/OS core. - The metronome service provides basic scheduling and job management similar to the cron program for the DC/OS cluster. As you deploy additional services, you can use the dcos service command to verify the status of those services.","title":"Verify services running on the cluster"},{"location":"cli/#check-the-status-of-connected-nodes","text":"You can use the DC/OS command-line interface to check the status of connected nodes and to return log information. As a starting point for exploring your DC/OS cluster, you might want to run the dcos node list command. dcos node list This command returns basic information about the connected agent and master nodes in your cluster. For example: HOSTNAME IP PUBLIC IP(S) ID TYPE REGION ZONE 10.0.2.246 10.0.2.246 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S1 agent aws/us-west-2 aws/us-west-2a 10.0.5.193 10.0.5.193 52.24.8.165 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S0 agent aws/us-west-2 aws/us-west-2a master.mesos. 10.0.4.82 34.220.80.239 ec31ddcf-1e31-4556-9f3b-9a56e172b6ef master (leader) aws/us-west-2 aws/us-west-2a ``` You might also want to explore the logs for individual nodes. For example, you can retrieve detailed information about the current master node leader by running the command `dcos node log --leader`. This command returns the information recorded in log messages for the leading master. The messages logged provide information to this sample entry: ```bash 2019-05-27 18:47:51 UTCbouncer.sh [2847]: 10.0.4.82 [27/May/2019:18:47:51 +0000] GET /acs/api/v1/internal/policyquery?rid=dcos:adminrouter:ops:mesos uid=dcos_history_service action=full HTTP/1.0 200 22 - Master Admin Router (0.001926 s) Similarly, you can retrieve detailed information about a specific node by running a command similar to the following: dcos node log --mesos-id ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S1 In this example, ec31ddcf-1e31-4556-9f3b-9a56e172b6ef-S1 is the agent identifier (ID). You can use the dcos node list command to look up the agent identifier, then retrieve log information for the agent. You can then use the information returned to trace agent activity, analyze operation, or troubleshoot potential issues.","title":"Check the status of connected nodes"},{"location":"cli/#getting-usage-information-for-cli-programs","text":"To explore the types of information available through the DC/OS CLI options, enter the dcos help command. This help option summarizes the top-level of commands available. For example: Usage: dcos [command] Commands: auth Authenticate to DC/OS cluster backup Access DC/OS backup functionality cluster Manage your DC/OS clusters config Manage the DC/OS configuration file help Help about any command job Deploy and manage jobs in DC/OS license Manage your DC/OS licenses marathon Deploy and manage applications to DC/OS node View DC/OS node information package Install and manage DC/OS software packages plugin Manage CLI plugins security DC/OS security related commands service Manage DC/OS services task Manage DC/OS tasks Options: --version Print version information -v, -vv Output verbosity (verbose or very verbose) -h, --help Show usage help You can then use the --help option for individual commands to view usage information about a specific command. For example, you can run dcos node --help to see information about specific dcos node commands and arguments. For more information about working with the DC/OS command-line interface, see the CLI documentation .","title":"Getting usage information for CLI programs"},{"location":"cli/#next-steps","text":"Congratulations! You have successfully connected to your cluster using the DC/OS CLI, and started exploring some of the tasks and information available using the DC/OS command-line interface. The next tutorials explore additional getting started tasks that you can perform using the DC/OS web-based administrative console or command-line interface: Install your first service from the package repository Deploy your first sample application Discover deployed services","title":"Next steps"},{"location":"cli/#related-topics","text":"You have already worked with several core components of the DC/OS architecture, including the Mesos kernel, Marathon, and Metronome. For more information about these DC/OS components and where they fit into the DC/OS platform or features and services, see the following topics in the main DC/OS documentation : Marathon starts and monitors DC/OS applications and services. Apache Mesos is the kernel of DC/OS and responsible for low-level task maintenance. Mesos DNS provides service discovery within the cluster. Minuteman is the internal layer 4 load balancer. Admin router is an open source NGINX configuration that provides central authentication and proxy to DC/OS services. Package repository is the package repository that holds the DC/OS services (e.g. Apache Spark or Apache Cassandra) that you can install on your cluster directly from the DC/OS GUI and CLI.","title":"Related topics"},{"location":"create-service/","text":"Create and run custom apps Illustrates how to create and deploy single command or an image using container This tutorial shows how to create and deploy a simple one-command service and a containerized service using the DC/OS web-based administrative console or by running command-line programs. Before you begin Before starting this tutorial, you should verify the following: You must be able to access a properly-configured and running DC/OS cluster . You must have the DC/OS command-line interface installed. Learning objectives By completing this tutorial, you will learn: How to use the DC/OS web-based console to create and deploy a single-command service. How to use the DC/OS command-line interface (CLI) to create and deploy a single-command service. How to use the DC/OS web-based console to create and deploy a containerized service. How to use the DC/OS command-line interface (CLI) to create and deploy a containerized service. Create a single command service You can create and deploy a simple single-command service using the DC/OS web-based administrative console or by running command-line programs. Using the DC/OS web-based console Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click Services , then click Run a Service . Click Single Container . Type a name for your service in the Service ID field. For example, type single-cmd-service for the service identifier. You can use the default values for the number of instances, CPUs, and memory and leave the Container Image field blank. For the Command field, enter sleep 10 echo DONE . Click More Settings , then select Universal Container Runtime (UCR) to use the native DC/OS container runtime. The Universal Container Runtime (UCR) supports Docker files, multiple containers (pods), and using GPU resources. If you select this option, you can optionally specify a Docker container image. You should only select the Docker Engine option if you need Docker-specific features. If you select this option, you must specify a Docker container image in the Container Image field. For this tutorial, you can leave the Advanced settings undefined. Click Review Run , then click Run Service . Click the name of your service in the Services view to see it running and monitor its health. Using the DC/OS CLI You can also create and run single-command services using the DC/OS CLI. Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Open a new file in a text editor to create a JSON file called single-cmd-app-cli.json . Copy and paste the following sample content into the single-cmd-app-cli.json file: json { \"id\": \"/single-cmd-app-cli\", \"cmd\": \"sleep 10\", \"instances\": 1, \"cpus\": 1, \"mem\": 128, \"portDefinitions\": [ { \"protocol\": \"tcp\", \"port\": 10000 } ], \"requirePorts\": false } Start the single-command service by running the following command: bash dcos marathon app add single-cmd-app-cli.json Verify the single-command service has been successfully deployed by running the following command: bash dcos marathon app list The command returns information similar to the following about the services you have deployed. ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /single-cmd-app-cli 128 1 0/1 N/A --- False N/A sleep 10 /single-cmd-service 128 0.1 0/1 N/A --- False MESOS sleep 10 echo Done As this sample output illustrates, you can verify that both the single-command service you created using the DC/OS web-based console and the one you deployed using a JSON file and CLI commands are running. You can also view both services by clicking Services in the DC/OS web-based console. Create a simple containerized service You can create and deploy containerized services using the DC/OS web-based administrative console or by running command-line programs. This exercise uses a sample containerized, long-running task that is available from the Mesosphere Docker Hub repository . Using the DC/OS web-based console Open a web browser and navigate to hello-dcos on the [Mesosphere Docker Hub repository and copy the latest image tag. Open a web browser and navigate to URL for the DC/OS web-based administrative console. Click Services , then click Run a Service . Click Single Container . Type a name for your service in the Service ID field. For example, type container-hello-dcos-service for the service identifier. Type the container path and image tag in the Container Image field. For example, type mesosphere/hello-dcos:1.0 where 1.0 is the image-tag you copied from to the hello-dcos page. Click Review Run , then click Run Service . Click the name of your service in the Services view to see it running and monitor its health. Click the name of the containerized service, then choose one task instance to view the task Details, Files, and Logs. Click Logs , then click Error (stderr) and Output (stdout) to see the logged messages and output for the containerized service. Using the DC/OS CLI Open a web browser and navigate to hello-dcos on the [Mesosphere Docker Hub repository and copy the latest image tag. Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Open a new file in a text editor to create a JSON file called container-app-cli.json . Copy and paste the following sample content into the container-app-cli.json file, replacing image-tag with the tag you copied in step 1. json { \"id\": \"/container-hello-dcos-cli\", \"instances\": 1, \"cpus\": 1, \"mem\": 128, \"container\": { \"type\": \"DOCKER\", \"docker\": { \"image\": \"mesosphere/hello-dcos: image-tag \", \"forcePullImage\": false, \"privileged\": false } }, \"acceptedResourceRoles\": [\"slave_public\"], \"portDefinitions\": [ { \"protocol\": \"tcp\", \"port\": 10001 } ], \"requirePorts\": false } Start the service by running the following command: bash dcos marathon app add container-app-cli.json Verify the containerized service has been successfully deployed by running the following command: bash dcos marathon app list The command returns information similar to the following about the services you have deployed. ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /container-hello-dcos-cli 128 1 1/1 N/A --- False DOCKER N/A /container-hello-dcos-service 128 0.1 1/1 N/A --- False DOCKER N/A /dcos-101/app1 128 1 1/1 N/A --- False DOCKER while true; do python... /redis-tutorial 1024 1 1/1 1/1 --- False DOCKER N/A /single-cmd-app-cli 128 1 0/1 N/A --- False N/A sleep 10 /single-cmd-service 128 0.1 0/1 N/A --- False MESOS sleep 10 echo DONE Open the DC/OS web-based administrative console, click Services , then click the name of the service to display its details. Click Logs , then click Output (stdout) view to see the output of the service. Next steps In this tutorial, you deployed some simple custom apps using both the DC/OS web-based administrative console and using DC/OS command-line programs. You have nown seen how to add apps to the cluster without using a container image, using the native Universal Container runtime, and using a Docker container image and have verified that all of the custom apps are running on the cluster. The next tutorials explore more advanced deployment scenarios and tasks and expose some additional core components of the DC/OS architecture: Discover deployed services Deploy native containerized applications Schedule tasks to run as jobs Related topics In this tutorial, you deployed custom apps using single containers and the [Marathon] orchestration framework. For more information about Universal Container Runtime and working with Docker containers and images, see Using Containerizers .","title":"Create and run custom apps"},{"location":"create-service/#create-and-run-custom-apps","text":"","title":"Create and run custom apps"},{"location":"create-service/#before-you-begin","text":"Before starting this tutorial, you should verify the following: You must be able to access a properly-configured and running DC/OS cluster . You must have the DC/OS command-line interface installed.","title":"Before you begin"},{"location":"create-service/#learning-objectives","text":"By completing this tutorial, you will learn: How to use the DC/OS web-based console to create and deploy a single-command service. How to use the DC/OS command-line interface (CLI) to create and deploy a single-command service. How to use the DC/OS web-based console to create and deploy a containerized service. How to use the DC/OS command-line interface (CLI) to create and deploy a containerized service.","title":"Learning objectives"},{"location":"create-service/#create-a-single-command-service","text":"You can create and deploy a simple single-command service using the DC/OS web-based administrative console or by running command-line programs.","title":"Create a single command service"},{"location":"create-service/#using-the-dcos-web-based-console","text":"Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click Services , then click Run a Service . Click Single Container . Type a name for your service in the Service ID field. For example, type single-cmd-service for the service identifier. You can use the default values for the number of instances, CPUs, and memory and leave the Container Image field blank. For the Command field, enter sleep 10 echo DONE . Click More Settings , then select Universal Container Runtime (UCR) to use the native DC/OS container runtime. The Universal Container Runtime (UCR) supports Docker files, multiple containers (pods), and using GPU resources. If you select this option, you can optionally specify a Docker container image. You should only select the Docker Engine option if you need Docker-specific features. If you select this option, you must specify a Docker container image in the Container Image field. For this tutorial, you can leave the Advanced settings undefined. Click Review Run , then click Run Service . Click the name of your service in the Services view to see it running and monitor its health.","title":"Using the DC/OS web-based console"},{"location":"create-service/#using-the-dcos-cli","text":"You can also create and run single-command services using the DC/OS CLI. Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Open a new file in a text editor to create a JSON file called single-cmd-app-cli.json . Copy and paste the following sample content into the single-cmd-app-cli.json file: json { \"id\": \"/single-cmd-app-cli\", \"cmd\": \"sleep 10\", \"instances\": 1, \"cpus\": 1, \"mem\": 128, \"portDefinitions\": [ { \"protocol\": \"tcp\", \"port\": 10000 } ], \"requirePorts\": false } Start the single-command service by running the following command: bash dcos marathon app add single-cmd-app-cli.json Verify the single-command service has been successfully deployed by running the following command: bash dcos marathon app list The command returns information similar to the following about the services you have deployed. ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /single-cmd-app-cli 128 1 0/1 N/A --- False N/A sleep 10 /single-cmd-service 128 0.1 0/1 N/A --- False MESOS sleep 10 echo Done As this sample output illustrates, you can verify that both the single-command service you created using the DC/OS web-based console and the one you deployed using a JSON file and CLI commands are running. You can also view both services by clicking Services in the DC/OS web-based console.","title":"Using the DC/OS CLI"},{"location":"create-service/#create-a-simple-containerized-service","text":"You can create and deploy containerized services using the DC/OS web-based administrative console or by running command-line programs. This exercise uses a sample containerized, long-running task that is available from the Mesosphere Docker Hub repository .","title":"Create a simple containerized service"},{"location":"create-service/#using-the-dcos-web-based-console_1","text":"Open a web browser and navigate to hello-dcos on the [Mesosphere Docker Hub repository and copy the latest image tag. Open a web browser and navigate to URL for the DC/OS web-based administrative console. Click Services , then click Run a Service . Click Single Container . Type a name for your service in the Service ID field. For example, type container-hello-dcos-service for the service identifier. Type the container path and image tag in the Container Image field. For example, type mesosphere/hello-dcos:1.0 where 1.0 is the image-tag you copied from to the hello-dcos page. Click Review Run , then click Run Service . Click the name of your service in the Services view to see it running and monitor its health. Click the name of the containerized service, then choose one task instance to view the task Details, Files, and Logs. Click Logs , then click Error (stderr) and Output (stdout) to see the logged messages and output for the containerized service.","title":"Using the DC/OS web-based console"},{"location":"create-service/#using-the-dcos-cli_1","text":"Open a web browser and navigate to hello-dcos on the [Mesosphere Docker Hub repository and copy the latest image tag. Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Open a new file in a text editor to create a JSON file called container-app-cli.json . Copy and paste the following sample content into the container-app-cli.json file, replacing image-tag with the tag you copied in step 1. json { \"id\": \"/container-hello-dcos-cli\", \"instances\": 1, \"cpus\": 1, \"mem\": 128, \"container\": { \"type\": \"DOCKER\", \"docker\": { \"image\": \"mesosphere/hello-dcos: image-tag \", \"forcePullImage\": false, \"privileged\": false } }, \"acceptedResourceRoles\": [\"slave_public\"], \"portDefinitions\": [ { \"protocol\": \"tcp\", \"port\": 10001 } ], \"requirePorts\": false } Start the service by running the following command: bash dcos marathon app add container-app-cli.json Verify the containerized service has been successfully deployed by running the following command: bash dcos marathon app list The command returns information similar to the following about the services you have deployed. ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /container-hello-dcos-cli 128 1 1/1 N/A --- False DOCKER N/A /container-hello-dcos-service 128 0.1 1/1 N/A --- False DOCKER N/A /dcos-101/app1 128 1 1/1 N/A --- False DOCKER while true; do python... /redis-tutorial 1024 1 1/1 1/1 --- False DOCKER N/A /single-cmd-app-cli 128 1 0/1 N/A --- False N/A sleep 10 /single-cmd-service 128 0.1 0/1 N/A --- False MESOS sleep 10 echo DONE Open the DC/OS web-based administrative console, click Services , then click the name of the service to display its details. Click Logs , then click Output (stdout) view to see the output of the service.","title":"Using the DC/OS CLI"},{"location":"create-service/#next-steps","text":"In this tutorial, you deployed some simple custom apps using both the DC/OS web-based administrative console and using DC/OS command-line programs. You have nown seen how to add apps to the cluster without using a container image, using the native Universal Container runtime, and using a Docker container image and have verified that all of the custom apps are running on the cluster. The next tutorials explore more advanced deployment scenarios and tasks and expose some additional core components of the DC/OS architecture: Discover deployed services Deploy native containerized applications Schedule tasks to run as jobs","title":"Next steps"},{"location":"create-service/#related-topics","text":"In this tutorial, you deployed custom apps using single containers and the [Marathon] orchestration framework. For more information about Universal Container Runtime and working with Docker containers and images, see Using Containerizers .","title":"Related topics"},{"location":"first-app/","text":"Deploy the first application Describes how to define and deploy a sample service instance on the cluster Now that you are familiar with how to search for and install services from the DC/OS package repository, you are ready to start deploying applications that use the service. This tutorial demonstrates how you can deploy a simple application that connects to the Redis service you deployed in the previous tutorial. Before you begin Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the Redis package installed and the Redis service running in your cluster. The sample application in this tutorial has some dependencies on external libraries. To ensure you can complete the tutorial, you should download the Docker image for the sample app. The Docker image provided includes all of the necessary files so that you don't need to download any additional libraries or resolve external dependencies. Learning objectives By completing this tutorial, you will learn: How to deploy a simple app definition that connects to the Redis service. How to check the status of your sample app. Review the sample app definition For this tutorial, you are going to deploy a very simple sample app that checks whether a connection to the Redis service is available, then prints the total number of keys stored there. The sample app is a Python script has a dependency on the redis-py Python library. Because of this dependency and that you cannot assume the required library will be present on all agent nodes, you should run the sample app using the dcos-101 Docker container that provides all of the dependencies. You might also want to review: The DOCKERFILE , used to create the dcos-101 image. The app definition used to deploy and manage the application. This app definition downloads the sample app Python script and runs it inside the dcos-101 container. Deploy the sample app Add the sample app to Marathon using the app definition by running the following command: dcos marathon app add https://raw.githubusercontent.com/joerg84/dcos-101/master/app1/app1.json Check that the sample app is running. From the DC/OS web-based console: Click Services . Click dcos-101 . Click app1 . From the DC/OS CLI: View the status for all DC/OS tasks by running: dcos task View information for all Marathon apps by running: dcos marathon app list View log information for the app by running: dcos task log app1 The output from the dcos task log app1 command indicates the node and port where the app1 sample application is running, the status of the connection to Redis, and the number of keys you have stored in the Redis service. For example: Running on node '10.4.6.52' and port '6512 Redis Connected. Total number of keys: 2 Keep in mind that the node and port information might vary between different runs and over the lifetime of the deployed app, depending on other events in the cluster. How DC/OS locates service instances running on different nodes and using different ports is covered in a later section. Next steps In this tutorial, you deployed your first app inside a Docker container using Marathon and verified that the app is running and that it can connect successfully to the previously-deployed Redis service. Taken together, these two tutorials represent a common scenario in which you deploy a backend service such as Redis or MySQL, then deploy apps that connect to that service to perform specific tasks such as reporting the number of keys stored in the Redis service or presenting the results of a query in a dashboard. The next tutorials explore additional deployment tasks that you can perform and expose additional components of the DC/OS architecture: Create and deploy your own apps Discover deployed services Deploy native containerized applications Schedule tasks to run as jobs Related topics You have now deployed the Redis service and a predefined sample app using Marathon . Marathon is a core component of the DC/OS platform. Marathon enables the DC/OS cluster to better support long-running services and is used to perform several key operations, including scaling up or down the number of app instances, modifying resource requires or configuration details, and deploying or removing applications from the cluster. For more information about working with Marathon, see the following topics: Deploying services and pods for informaiton about using Marathon to manage your processes, services, and multiple service pods. DC/OS CLI Marathon plugin for information about using DC/OS CLI commands for Marathon. You can also get more information about dcos marathon commands by typing dcos marathon app --help in a terminal shell. REST API for information about using HTTP endpoints.","title":"Deploy the first app"},{"location":"first-app/#deploy-the-first-application","text":"","title":"Deploy the first application"},{"location":"first-app/#before-you-begin","text":"Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the Redis package installed and the Redis service running in your cluster. The sample application in this tutorial has some dependencies on external libraries. To ensure you can complete the tutorial, you should download the Docker image for the sample app. The Docker image provided includes all of the necessary files so that you don't need to download any additional libraries or resolve external dependencies.","title":"Before you begin"},{"location":"first-app/#learning-objectives","text":"By completing this tutorial, you will learn: How to deploy a simple app definition that connects to the Redis service. How to check the status of your sample app.","title":"Learning objectives"},{"location":"first-app/#review-the-sample-app-definition","text":"For this tutorial, you are going to deploy a very simple sample app that checks whether a connection to the Redis service is available, then prints the total number of keys stored there. The sample app is a Python script has a dependency on the redis-py Python library. Because of this dependency and that you cannot assume the required library will be present on all agent nodes, you should run the sample app using the dcos-101 Docker container that provides all of the dependencies. You might also want to review: The DOCKERFILE , used to create the dcos-101 image. The app definition used to deploy and manage the application. This app definition downloads the sample app Python script and runs it inside the dcos-101 container.","title":"Review the sample app definition"},{"location":"first-app/#deploy-the-sample-app","text":"Add the sample app to Marathon using the app definition by running the following command: dcos marathon app add https://raw.githubusercontent.com/joerg84/dcos-101/master/app1/app1.json Check that the sample app is running. From the DC/OS web-based console: Click Services . Click dcos-101 . Click app1 . From the DC/OS CLI: View the status for all DC/OS tasks by running: dcos task View information for all Marathon apps by running: dcos marathon app list View log information for the app by running: dcos task log app1 The output from the dcos task log app1 command indicates the node and port where the app1 sample application is running, the status of the connection to Redis, and the number of keys you have stored in the Redis service. For example: Running on node '10.4.6.52' and port '6512 Redis Connected. Total number of keys: 2 Keep in mind that the node and port information might vary between different runs and over the lifetime of the deployed app, depending on other events in the cluster. How DC/OS locates service instances running on different nodes and using different ports is covered in a later section.","title":"Deploy the sample app"},{"location":"first-app/#next-steps","text":"In this tutorial, you deployed your first app inside a Docker container using Marathon and verified that the app is running and that it can connect successfully to the previously-deployed Redis service. Taken together, these two tutorials represent a common scenario in which you deploy a backend service such as Redis or MySQL, then deploy apps that connect to that service to perform specific tasks such as reporting the number of keys stored in the Redis service or presenting the results of a query in a dashboard. The next tutorials explore additional deployment tasks that you can perform and expose additional components of the DC/OS architecture: Create and deploy your own apps Discover deployed services Deploy native containerized applications Schedule tasks to run as jobs","title":"Next steps"},{"location":"first-app/#related-topics","text":"You have now deployed the Redis service and a predefined sample app using Marathon . Marathon is a core component of the DC/OS platform. Marathon enables the DC/OS cluster to better support long-running services and is used to perform several key operations, including scaling up or down the number of app instances, modifying resource requires or configuration details, and deploying or removing applications from the cluster. For more information about working with Marathon, see the following topics: Deploying services and pods for informaiton about using Marathon to manage your processes, services, and multiple service pods. DC/OS CLI Marathon plugin for information about using DC/OS CLI commands for Marathon. You can also get more information about dcos marathon commands by typing dcos marathon app --help in a terminal shell. REST API for information about using HTTP endpoints.","title":"Related topics"},{"location":"first-package/","text":"Install the first package Illustrates how to install a sample service package Now that you have a DC/OS cluster installed and running on master and agent nodes and have installed the DC/OS command-line interface (CLI) to work with your cluster, you are ready to begin adding packages and applications to the cluster. Before you begin Before starting this tutorial, you should verify the following: - You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You can open a command-line shell on the computer hosting the CLI. You can open secure shell (SSH) sessions on remote cluster nodes. !-- To perform some steps in this tutorial, you also need access to the jq language JSON processor to simplify some of the commands and output format. Download jq and follow the instructions to install JQ for your operating system.-- Learning objective By completing this tutorial, you will learn: - How to search for services in the DC/OS package repository. How to install a service you want available in the DC/OS cluster. How to run a few basic commands for working with your first service. Search for a package For this tutorial, you are going to install Redis . Redis is an open source key-value data structure store. It is commonly used as a database, cache manager, and message broker. It supports in-memory data retrieval, on-disk persistence, and high availability. You can search for packages you want to install on the DC/OS cluster by using the DC/OS web-based administrative console or by running DC/OS command-line programs. Search using the DC/OS web-based console To search for Redis using the DC/OS web-based administrative console: 1. Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click Catalog . The Catalog provides a package repository for services that are available for DC/OS clusters. If you have an Internet connection, the Catalog makes easy to install services with minimal manual configuration from a centralized location. The packages in the Catalog are developed and maintained by many different contributors and include both Certified packages and that have been tested and validated by Mesosphere and Community that have been contributed to the package repository but in many cases have not been thoroughly tested. Type a search string to locate the package you want to install. For example, type \"redis\" to find the package names that match the package you are going to install for this tutorial. In this case, more than one package matches your search string. For this tutorial, however, you are only interested in the redis package. This package installs a single Redis instance in a Docker container. Select the Redis package in the search results. If you are ready to install using the DC/OS web-based administrative console, continue to Install using the DC/OS web-based console . Search using the DC/OS CLI To search for Redis by running DC/OS CLI commands: 1. Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Search for the package by running the following command: bash dcos package search redis Review the output for the command. For example, this command returns the following entries: NAME VERSION SELECTED FRAMEWORK DESCRIPTION mr-redis 0.0.1 False True Redis is the fastest in-memory KV-Cache and Datatstructure store redis 4.0-0.0.1 False False This is a single redis container, which is NOT suited for HA setups. Redis is... Install the package For this tutorial, you are only interested in the redis package. This package installs a single Redis instance in a Docker container. You can install this package using the DC/OS web-based administrative console or by running command-line program. Install using the DC/OS web-based console To install the Redis package using the DC/OS web-based administrative console: 1. Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click Catalog . Scroll or search to locate the Redis package you want to install. For this tutorial, select the redis package. This package installs a single Redis instance in a Docker container. Click Review Run . Verify the default service name. You can modify the service name, if needed. For example, you might want to name this service redis-tutorial . Click Redis and verify the CPU and memory settings. Click Review Run to verify your Redis configuration, then click Run Service . Click Open Service to view the status of the Redis deployment. Install using the DC/OS CLI Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Install the Redis package by running the following command: bash dcos package install redis --yes Verify the service is installed and running You can verify that the Redis service is currently running and reporting a Healthy status using the DC/OS web-based administrative console or by running command-line programs. Check Redis status in the DC/OS web-based console Open the DC/OS web-based administrative console. Click Services to view the list of deployed services. Verify the Status column for Redis displays Running. Click the service name to display task-level details. Check Redis status using DC/OS commands Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Check DC/OS task information by running the following command: dcos task This command displays basic information for all running DC/OS tasks. For example: NAME HOST USER STATE ID MESOS ID REGION ZONE redis-tutorial 10.0.1.192 root R redis-tutorial.instance-f4adfcf2-830c-11e9-9380-d281e1886025._app.1 da7a5a1b-ee52-4127-baf2-4989ba6fffea-S1 aws/us-west-2 aws/us-west-2a Review information for all deployed Marathon apps by running the following command: dcos marathon app list Because Marathon is used to start the Redis service, Redis is listed in the output for this command. Notice that the Health column indicates that Redis is configured to run one instance and one instance is currently running (1/1). ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /redis-tutorial 1024 1 1/1 1/1 --- False DOCKER N/A Check the Redis log file by running the following command: dcos task log redis This command displays the standard output (stdout) and standard error (stderr) logs for the Redis task. The log file output enables you to check whether the actual startup was successful. By default, the command displays the last 10 lines of logged activity. You can change the number of log lines displayed by specifying the --lines= argument. Test service operations Now that you have installed the Redis package, deployed the service on the cluster, and verified that the service is healthy, you can complete this tutorial by using Redis to store a key-value pair manually using the redis-cli command. Open a terminal shell on a computer with network access to the cluster. Open a secure shell (SSH) session on the cluster node where the Redis service is running. There are several ways you can determine the cluster node address and Mesos task identifier for the Redis service running on that node. For example, if you know the host name or IP address, have the appropriate login credentials, and are authorized to use SSH to connect to the computer, you can access the node by running a command similar to the following: ssh agent-node-ip -l authorized-user You can also use dcos task to look up the Mesos ID for the Redis service, then open a secure shell using a command similar to the following: dcos node ssh --master-proxy --mesos-id=dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 List the Docker containers to get the ContainerID for the container running the Redis service by running the following command: sudo docker ps This command returns output similar to the following: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 296b18087535 redis:4.0 \"docker-entrypoint.s\u2026\" About an hour ago Up About an hour 0.0.0.0:3617- 6379/tcp mesos-6e81fd7b-9fa8-470d-9378-49b4a01b2d11 Start a shell session in the running container by running the following command and substituting the CONTAINER_ID argument with the container ID you got from running the docker ps command: sudo docker exec -i -t CONTAINER_ID /bin/bash Start the Redis CLI client by running the following command: redis-cli Set a key with a value by running the following command: set tutorial my-tutorial-key-value Verify the key-value pair by running the following command: get tutorial Add additional keys using the redis-cli , if needed. In the next tutorial, you will deploy a simple application that connects to the Redis service and retrieves the number of keys defined. Exit the redis-cli client and end the secure shell session. Next steps You have successfully installed your first service from the package repository and verified it is running. The next tutorials explore additional getting started tasks that you can perform using the DC/OS web-based administrative console or command-line interface: Deploy your first sample application Create and run custom apps Discover deployed services Deploy native containerized applications Related topics The Mesosphere Catalog (or Universe in previous versions of DC/OS) is a package repository for services that are available for installation on DC/OS clusters. The package repository enables you to easily install certified or community-contributed services, such as Apache Spark or Apache Cassandra, in your cluster without having to locate, download, and configure independent packages manually. If your cluster runs on an isolated network without an internet connection, you can create and manage your own site-specific package repository. For information about creating your own package repository that includes your custom packages, see Deploying a local Universe for details.","title":"Install the first package"},{"location":"first-package/#install-the-first-package","text":"","title":"Install the first package"},{"location":"first-package/#before-you-begin","text":"Before starting this tutorial, you should verify the following: - You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You can open a command-line shell on the computer hosting the CLI. You can open secure shell (SSH) sessions on remote cluster nodes. !-- To perform some steps in this tutorial, you also need access to the jq language JSON processor to simplify some of the commands and output format. Download jq and follow the instructions to install JQ for your operating system.--","title":"Before you begin"},{"location":"first-package/#learning-objective","text":"By completing this tutorial, you will learn: - How to search for services in the DC/OS package repository. How to install a service you want available in the DC/OS cluster. How to run a few basic commands for working with your first service.","title":"Learning objective"},{"location":"first-package/#search-for-a-package","text":"For this tutorial, you are going to install Redis . Redis is an open source key-value data structure store. It is commonly used as a database, cache manager, and message broker. It supports in-memory data retrieval, on-disk persistence, and high availability. You can search for packages you want to install on the DC/OS cluster by using the DC/OS web-based administrative console or by running DC/OS command-line programs.","title":"Search for a package"},{"location":"first-package/#search-using-the-dcos-web-based-console","text":"To search for Redis using the DC/OS web-based administrative console: 1. Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click Catalog . The Catalog provides a package repository for services that are available for DC/OS clusters. If you have an Internet connection, the Catalog makes easy to install services with minimal manual configuration from a centralized location. The packages in the Catalog are developed and maintained by many different contributors and include both Certified packages and that have been tested and validated by Mesosphere and Community that have been contributed to the package repository but in many cases have not been thoroughly tested. Type a search string to locate the package you want to install. For example, type \"redis\" to find the package names that match the package you are going to install for this tutorial. In this case, more than one package matches your search string. For this tutorial, however, you are only interested in the redis package. This package installs a single Redis instance in a Docker container. Select the Redis package in the search results. If you are ready to install using the DC/OS web-based administrative console, continue to Install using the DC/OS web-based console .","title":"Search using the DC/OS web-based console"},{"location":"first-package/#search-using-the-dcos-cli","text":"To search for Redis by running DC/OS CLI commands: 1. Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Search for the package by running the following command: bash dcos package search redis Review the output for the command. For example, this command returns the following entries: NAME VERSION SELECTED FRAMEWORK DESCRIPTION mr-redis 0.0.1 False True Redis is the fastest in-memory KV-Cache and Datatstructure store redis 4.0-0.0.1 False False This is a single redis container, which is NOT suited for HA setups. Redis is...","title":"Search using the DC/OS CLI"},{"location":"first-package/#install-the-package","text":"For this tutorial, you are only interested in the redis package. This package installs a single Redis instance in a Docker container. You can install this package using the DC/OS web-based administrative console or by running command-line program.","title":"Install the package"},{"location":"first-package/#install-using-the-dcos-web-based-console","text":"To install the Redis package using the DC/OS web-based administrative console: 1. Open a web browser and navigate to the URL for the DC/OS web-based administrative console. Click Catalog . Scroll or search to locate the Redis package you want to install. For this tutorial, select the redis package. This package installs a single Redis instance in a Docker container. Click Review Run . Verify the default service name. You can modify the service name, if needed. For example, you might want to name this service redis-tutorial . Click Redis and verify the CPU and memory settings. Click Review Run to verify your Redis configuration, then click Run Service . Click Open Service to view the status of the Redis deployment.","title":"Install using the DC/OS web-based console"},{"location":"first-package/#install-using-the-dcos-cli","text":"Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Install the Redis package by running the following command: bash dcos package install redis --yes","title":"Install using the DC/OS CLI"},{"location":"first-package/#verify-the-service-is-installed-and-running","text":"You can verify that the Redis service is currently running and reporting a Healthy status using the DC/OS web-based administrative console or by running command-line programs.","title":"Verify the service is installed and running"},{"location":"first-package/#check-redis-status-in-the-dcos-web-based-console","text":"Open the DC/OS web-based administrative console. Click Services to view the list of deployed services. Verify the Status column for Redis displays Running. Click the service name to display task-level details.","title":"Check Redis status in the DC/OS web-based console"},{"location":"first-package/#check-redis-status-using-dcos-commands","text":"Open a terminal shell on the computer where you have access to the DC/OS command-line interface (CLI). Check DC/OS task information by running the following command: dcos task This command displays basic information for all running DC/OS tasks. For example: NAME HOST USER STATE ID MESOS ID REGION ZONE redis-tutorial 10.0.1.192 root R redis-tutorial.instance-f4adfcf2-830c-11e9-9380-d281e1886025._app.1 da7a5a1b-ee52-4127-baf2-4989ba6fffea-S1 aws/us-west-2 aws/us-west-2a Review information for all deployed Marathon apps by running the following command: dcos marathon app list Because Marathon is used to start the Redis service, Redis is listed in the output for this command. Notice that the Health column indicates that Redis is configured to run one instance and one instance is currently running (1/1). ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /redis-tutorial 1024 1 1/1 1/1 --- False DOCKER N/A Check the Redis log file by running the following command: dcos task log redis This command displays the standard output (stdout) and standard error (stderr) logs for the Redis task. The log file output enables you to check whether the actual startup was successful. By default, the command displays the last 10 lines of logged activity. You can change the number of log lines displayed by specifying the --lines= argument.","title":"Check Redis status using DC/OS commands"},{"location":"first-package/#test-service-operations","text":"Now that you have installed the Redis package, deployed the service on the cluster, and verified that the service is healthy, you can complete this tutorial by using Redis to store a key-value pair manually using the redis-cli command. Open a terminal shell on a computer with network access to the cluster. Open a secure shell (SSH) session on the cluster node where the Redis service is running. There are several ways you can determine the cluster node address and Mesos task identifier for the Redis service running on that node. For example, if you know the host name or IP address, have the appropriate login credentials, and are authorized to use SSH to connect to the computer, you can access the node by running a command similar to the following: ssh agent-node-ip -l authorized-user You can also use dcos task to look up the Mesos ID for the Redis service, then open a secure shell using a command similar to the following: dcos node ssh --master-proxy --mesos-id=dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 List the Docker containers to get the ContainerID for the container running the Redis service by running the following command: sudo docker ps This command returns output similar to the following: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 296b18087535 redis:4.0 \"docker-entrypoint.s\u2026\" About an hour ago Up About an hour 0.0.0.0:3617- 6379/tcp mesos-6e81fd7b-9fa8-470d-9378-49b4a01b2d11 Start a shell session in the running container by running the following command and substituting the CONTAINER_ID argument with the container ID you got from running the docker ps command: sudo docker exec -i -t CONTAINER_ID /bin/bash Start the Redis CLI client by running the following command: redis-cli Set a key with a value by running the following command: set tutorial my-tutorial-key-value Verify the key-value pair by running the following command: get tutorial Add additional keys using the redis-cli , if needed. In the next tutorial, you will deploy a simple application that connects to the Redis service and retrieves the number of keys defined. Exit the redis-cli client and end the secure shell session.","title":"Test service operations"},{"location":"first-package/#next-steps","text":"You have successfully installed your first service from the package repository and verified it is running. The next tutorials explore additional getting started tasks that you can perform using the DC/OS web-based administrative console or command-line interface: Deploy your first sample application Create and run custom apps Discover deployed services Deploy native containerized applications","title":"Next steps"},{"location":"first-package/#related-topics","text":"The Mesosphere Catalog (or Universe in previous versions of DC/OS) is a package repository for services that are available for installation on DC/OS clusters. The package repository enables you to easily install certified or community-contributed services, such as Apache Spark or Apache Cassandra, in your cluster without having to locate, download, and configure independent packages manually. If your cluster runs on an isolated network without an internet connection, you can create and manage your own site-specific package repository. For information about creating your own package repository that includes your custom packages, see Deploying a local Universe for details.","title":"Related topics"},{"location":"loadbalancing/","text":"Load balancing workloads Demonstrates distributing workload for clients requesting access to a deployed application Conceptually, a load balancer provides a network communication layer for distributing client requests to applications. Load balancers are particularly important in a clustered network environment because they determine which instance of an application should respond to internal or external service requests. With DC/OS clusters, you have several options for distributing service requests through a load balancer: You can use named virtual IP addresses and the native distributed layer-4 load balancing service dcos-l4lb (previously known as Minuteman). You can use the Marathon-LB implementation of the [HAProxy] open-source load balancing service if all of your deployed applications use Marathon app definitions. You can use Edge-LB layer-7 proxy and load balancing service to distribute inbound access requests for a DC/OS Enterprise cluster. You can use an external hardware or cloud provider load balancing service, such as the AWS Elastic Load Balancer (ELB) in combination with Marathon-LB or Edge-LB for multi-tiered routing of inbound requests. You have already explored two of these load balancing options in previous tutorials that illustrated service discovery , and using Marathon-LB to expose an application on a public agent node . Because this is a key task for managing cluster operations, this tutorial provides a more detailed example of how to configure load balancing for the simple apps you've deployed so far. Before you begin Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the app2 sample application deployed and running in your cluster. You have Marathon-LB deployed and running in your cluster. Learning objectives By completing this tutorial, you will learn: How to scale your application to multiple instances. How internal and external services choose which instance to use once the application has been scaled. Add application instances Scale the app2 sample application to two instances by running the following command: bash dcos marathon app update /dcos-101/app2 instances=2 Load balancing with Marathon-LB Check that you can access the application using the public agent node IP address and port 10000 by opening a web browser and navigating to the URL. For example, navigate to a URL similar to this: http:// public-node :10000 . Navigate to the application repeatedly to see the request being served by different instances of app2. Check the Marathon-LB statistics by accessing the HAProxy stats endpoint using a URL similar to this: http:// public-node :9090/haproxy?stats . Load balancing with virtual IPs addresses Open a terminal and use a secure shell (SSH) to connect to the leading master node by running the following command: bash dcos node ssh --master-proxy --leader Use cURL to get the raw HTML output from the application by running the following command: bash curl dcos-101app2.marathon.l4lb.thisdcos.directory:10000 Repeat the previous step to see the request being served by different instances. Remove extra application instances Scale the app2 application back to one instance by running the following command: bash dcos marathon app update /dcos-101/app2 instances=1 Next steps You have now used Marathon-LB and virtual IP addresses to load balance requests for two different instances of your app. Related topics Consider these features and benefits when choosing the load balancing mechanism. Marathon-LB is a layer 7 load balancer that is mostly used for external requests. It is based on the well-known HAProxy load balancer and uses Marathon\u2019s event bus to update its configuration in real time. Being a layer 7 load balancer, it supports session-based features such as HTTP sticky sessions and zero-downtime deployments. Named virtual IP addresses are a layer 4 load balancer mechanism used for internal TCP traffic. As they are tightly integrated with the kernel, they provide a load balanced IP address which can be used from anywhere within the cluster.","title":"Load balancing workload"},{"location":"loadbalancing/#load-balancing-workloads","text":"","title":"Load balancing workloads"},{"location":"loadbalancing/#before-you-begin","text":"Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the app2 sample application deployed and running in your cluster. You have Marathon-LB deployed and running in your cluster.","title":"Before you begin"},{"location":"loadbalancing/#learning-objectives","text":"By completing this tutorial, you will learn: How to scale your application to multiple instances. How internal and external services choose which instance to use once the application has been scaled.","title":"Learning objectives"},{"location":"loadbalancing/#add-application-instances","text":"Scale the app2 sample application to two instances by running the following command: bash dcos marathon app update /dcos-101/app2 instances=2","title":"Add application instances"},{"location":"loadbalancing/#load-balancing-with-marathon-lb","text":"Check that you can access the application using the public agent node IP address and port 10000 by opening a web browser and navigating to the URL. For example, navigate to a URL similar to this: http:// public-node :10000 . Navigate to the application repeatedly to see the request being served by different instances of app2. Check the Marathon-LB statistics by accessing the HAProxy stats endpoint using a URL similar to this: http:// public-node :9090/haproxy?stats .","title":"Load balancing with Marathon-LB"},{"location":"loadbalancing/#load-balancing-with-virtual-ips-addresses","text":"Open a terminal and use a secure shell (SSH) to connect to the leading master node by running the following command: bash dcos node ssh --master-proxy --leader Use cURL to get the raw HTML output from the application by running the following command: bash curl dcos-101app2.marathon.l4lb.thisdcos.directory:10000 Repeat the previous step to see the request being served by different instances.","title":"Load balancing with virtual IPs addresses"},{"location":"loadbalancing/#remove-extra-application-instances","text":"Scale the app2 application back to one instance by running the following command: bash dcos marathon app update /dcos-101/app2 instances=1","title":"Remove extra application instances"},{"location":"loadbalancing/#next-steps","text":"You have now used Marathon-LB and virtual IP addresses to load balance requests for two different instances of your app.","title":"Next steps"},{"location":"loadbalancing/#related-topics","text":"Consider these features and benefits when choosing the load balancing mechanism. Marathon-LB is a layer 7 load balancer that is mostly used for external requests. It is based on the well-known HAProxy load balancer and uses Marathon\u2019s event bus to update its configuration in real time. Being a layer 7 load balancer, it supports session-based features such as HTTP sticky sessions and zero-downtime deployments. Named virtual IP addresses are a layer 4 load balancer mechanism used for internal TCP traffic. As they are tightly integrated with the kernel, they provide a load balanced IP address which can be used from anywhere within the cluster.","title":"Related topics"},{"location":"native-app/","text":"Deploy and expose native applications Deploys an app using a UCR container and exposes it for access from outside of the cluster In a previous tutorial , you deployed an application that runs inside the cluster and interacts with another application--the Redis service--that also runs inside the cluster. Neither application is exposed outside of the cluster or available to any external users. This is because DC/OS supports running applications on two different type of nodes: private agent nodes and public agent nodes . So far, you have only worked with applications and services that run on private agent nodes, which cannot be accessed from outside of the cluster. To expose a service or application to the outside world, you typically use a load balancer running on a public node. In this tutorial, you will deploy another sample application but with a few important differences: The new sample application includes a presentation layer that presents a web-based user interface to users who access the application. The new sample application uses a native DC/OS container--the Universal Container Runtime (UCR)--that does not rely on a Docker image or the Docker engine, making the application easier to deploy with fewer dependencies and less complexity. You will expose the new sample application for access from outside of the cluster by running it on a public agent node with Marathon-LB as the load balancer. Before you begin Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the sample dcos-101/app1 application deployed and running in your cluster. Learning objectives By completing this tutorial, you will learn: How to deploy an app that uses the DC/OS Universal Container Runtime instead of Docker. How to make an app available to clients outside of the cluster by running it on a public agent node using a public-facing IP address and the Marathon-LB load balancer. How to test access to the new sample app. Review the sample application The app2 sample application is a [Go-based]](https://golang.org/) HTTP server that exposes a simple interface to Redis. If you review the app definition , you can see that this sample app is a binary without any external dependencies. Because it has no external dependencies, you can deploy it using a DC/OS native Universal Container Runtime (UCR) container. Deploy the sample app Deploy the sample application by running the following command: bash dcos marathon app add https://raw.githubusercontent.com/joerg84/dcos-101/master/app2/app2.json Alternatively, you can deploy the sample app from the DC/OS web-based administrative console. Verify the new sample app deployed successfully by running the following command to list all DC/OS tasks: bash dcos task The command returns output similar to the following: bash NAME HOST USER STATE ID MESOS ID REGION ZONE app2.dcos-101 10.0.1.127 root R dcos-101_app2.instance-d86ffa58-8935-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 aws/us-west-2 aws/us-west-2a You can also verify a successful deployment by running the following command to list all Marathon apps: bash dcos marathon app list ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /dcos-101/app2 128 1 1/1 N/A --- False N/A chmod u+x app2 ./app2 Test the HTTP server from within the cluster by connecting to the leading master and running the client URL ( cURL ) command: bash dcos node ssh --master-proxy --leader curl dcos-101app2.marathon.l4lb.thisdcos.directory:10000 The cURL command returns the raw HTML response from app2's web server with output similar to the following: html title Welcome to DC/OS 101! /title body h1 Welcome to DC/OS 101! /h1 h1 Running on node '10.0.1.127' and port '26962' /h1 h1 Add a new key:value pair /h1 form action=\"/save\" method=\"POST\" textarea name=\"key\" Key /textarea br textarea name=\"value\" Value /textarea br input type=\"submit\" value=\"Save\" /form /body /html Accessing the app from within the cluster and viewing the raw HTML response proves the application is running. For this tutorial, however, you also want to expose the app to the public. In the next part of this tutorial you will do exactly that. Install the load balancer Public agent nodes allow inbound access requests from clients outside of the cluster. The public agent is exposed to the outside world through a load balancer. For this tutorial, you will install Marathon-LB as the load balancer to provide external access for applications running internally in the cluster. Install Marathon-LB by running the following command: bash dcos package install marathon-lb --yes Verify that Marathon-LB was successfully deployed by running the following command: ```bash dcos task NAME HOST USER STATE ID MESOS ID REGION ZONE app2.dcos-101 10.0.1.127 root R dcos-101_app2.instance-d86ffa58-8935-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 aws/us-west-2 aws/us-west-2a marathon-lb 10.0.7.218 root R marathon-lb.instance-0ffbfc6c-8942-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S1 aws/us-west-2 aws/us-west-2a redis-tutorial 10.0.1.127 root R redis-tutorial.instance-97dae2d7-8934-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 aws/us-west-2 aws/us-west-2a ``` If your cluster uses a cloud provider such as AWS, the dcos task might show you the private IP address of the host, which is not resolvable from outside the cluster. Using the output from the dcos task command, however, you can determine the private IP assigned to the marathon-lb task. In this example, the private IP address is 10.0.7.218. Identify the IP address of the public agent node that Marathon-LB is using by running the following command: bash dcos node list The command returns information similar to the following: HOSTNAME IP PUBLIC IP(S) ID TYPE REGION ZONE 10.0.7.218 10.0.7.218 34.214.200.181 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S1 agent aws/us-west-2 aws/us-west-2a 10.0.1.127 10.0.1.127 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 agent aws/us-west-2 aws/us-west-2a master.mesos. 10.0.7.173 34.219.206.248 dedbb786-feb7-47f2-ae69-27bf86ba53fb master (leader) aws/us-west-2 aws/us-west-2a From the output for the dcos node list command, you can see the public IP address that corresponds with the private IP address where the marathon-lb task is running. In this example, the public IP address for the Marathon-LB service is 34.214.200.181. Connect using the public IP address Connect to the web app from your local computer using the public IP address and port 10000. For example: 34.214.200.181:10000 . You should see a simple web page form similar to this: Add a new Key and add a new Value, then click Save using the sample app web-based frontend. Verify the total number of keys using the app1 sample application by running the dcos task log app1 command. Check Redis directly by running dcos task , copying the Mesos ID returned for the Redis service, then opening a secure shell (SSH) on the node where the Redis service is running. For example, if the dcos task output displays dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 for the Redis task Mesos ID column, you can connect to the node using the dcos node ssh --master-proxy --mesos-id=dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 command, then so the following: List the Docker containers for the agent using docker ps . Copy the ContainerID for the Redis task from the output for the docker ps command. Create a bash session in the Docker container using the ContainerID from the previous and running sudo docker exec -i -t CONTAINER_ID /bin/bash . Start the Redis CLI by running redis-cli in the bash shell. Check for the key value you added by running the get newkey command. Next steps Congratulations! You have deployed a sample application that uses the native DC/OS UCR container, used Marathon-LB to expose the application to the public, and tested your publicly-available app by adding a new key to the Redis service using the web frontend. Related topics DC/OS uses containerizers to run tasks in containers. Running tasks in containers enables you to isolate tasks from each other and control task resources programmatically. DC/OS supports two types of containerizers - the DC/OS Universal Container Runtime, and the Docker containerizer. At this point, you have seen how to deployed apps using a Docker image (app1) and using the native Universal Container Runtime (app2). For your first app, you used a Docker container image to package dependencies so that you didn\u2019t need to rely on particular programs being available on the agent, then used the Docker containerizer to run the app packaged in the Docker image. Because the Docker containerizer internally uses the Docker runtime , you also used the Docker runtime. For your second app, you did not have any dependencies and therefore could rely on the default DC/OS Universal Container Runtime. Internally, both runtimes use the same OS features for isolation, namely cgroups and namespaces .","title":"Deploy and expose native container apps"},{"location":"native-app/#deploy-and-expose-native-applications","text":"","title":"Deploy and expose native applications"},{"location":"native-app/#before-you-begin","text":"Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the sample dcos-101/app1 application deployed and running in your cluster.","title":"Before you begin"},{"location":"native-app/#learning-objectives","text":"By completing this tutorial, you will learn: How to deploy an app that uses the DC/OS Universal Container Runtime instead of Docker. How to make an app available to clients outside of the cluster by running it on a public agent node using a public-facing IP address and the Marathon-LB load balancer. How to test access to the new sample app.","title":"Learning objectives"},{"location":"native-app/#review-the-sample-application","text":"The app2 sample application is a [Go-based]](https://golang.org/) HTTP server that exposes a simple interface to Redis. If you review the app definition , you can see that this sample app is a binary without any external dependencies. Because it has no external dependencies, you can deploy it using a DC/OS native Universal Container Runtime (UCR) container.","title":"Review the sample application"},{"location":"native-app/#deploy-the-sample-app","text":"Deploy the sample application by running the following command: bash dcos marathon app add https://raw.githubusercontent.com/joerg84/dcos-101/master/app2/app2.json Alternatively, you can deploy the sample app from the DC/OS web-based administrative console. Verify the new sample app deployed successfully by running the following command to list all DC/OS tasks: bash dcos task The command returns output similar to the following: bash NAME HOST USER STATE ID MESOS ID REGION ZONE app2.dcos-101 10.0.1.127 root R dcos-101_app2.instance-d86ffa58-8935-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 aws/us-west-2 aws/us-west-2a You can also verify a successful deployment by running the following command to list all Marathon apps: bash dcos marathon app list ID MEM CPUS TASKS HEALTH DEPLOYMENT WAITING CONTAINER CMD /dcos-101/app2 128 1 1/1 N/A --- False N/A chmod u+x app2 ./app2 Test the HTTP server from within the cluster by connecting to the leading master and running the client URL ( cURL ) command: bash dcos node ssh --master-proxy --leader curl dcos-101app2.marathon.l4lb.thisdcos.directory:10000 The cURL command returns the raw HTML response from app2's web server with output similar to the following: html title Welcome to DC/OS 101! /title body h1 Welcome to DC/OS 101! /h1 h1 Running on node '10.0.1.127' and port '26962' /h1 h1 Add a new key:value pair /h1 form action=\"/save\" method=\"POST\" textarea name=\"key\" Key /textarea br textarea name=\"value\" Value /textarea br input type=\"submit\" value=\"Save\" /form /body /html Accessing the app from within the cluster and viewing the raw HTML response proves the application is running. For this tutorial, however, you also want to expose the app to the public. In the next part of this tutorial you will do exactly that.","title":"Deploy the sample app"},{"location":"native-app/#install-the-load-balancer","text":"Public agent nodes allow inbound access requests from clients outside of the cluster. The public agent is exposed to the outside world through a load balancer. For this tutorial, you will install Marathon-LB as the load balancer to provide external access for applications running internally in the cluster. Install Marathon-LB by running the following command: bash dcos package install marathon-lb --yes Verify that Marathon-LB was successfully deployed by running the following command: ```bash dcos task NAME HOST USER STATE ID MESOS ID REGION ZONE app2.dcos-101 10.0.1.127 root R dcos-101_app2.instance-d86ffa58-8935-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 aws/us-west-2 aws/us-west-2a marathon-lb 10.0.7.218 root R marathon-lb.instance-0ffbfc6c-8942-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S1 aws/us-west-2 aws/us-west-2a redis-tutorial 10.0.1.127 root R redis-tutorial.instance-97dae2d7-8934-11e9-a1c1-4a501e74c1fd._app.1 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 aws/us-west-2 aws/us-west-2a ``` If your cluster uses a cloud provider such as AWS, the dcos task might show you the private IP address of the host, which is not resolvable from outside the cluster. Using the output from the dcos task command, however, you can determine the private IP assigned to the marathon-lb task. In this example, the private IP address is 10.0.7.218. Identify the IP address of the public agent node that Marathon-LB is using by running the following command: bash dcos node list The command returns information similar to the following: HOSTNAME IP PUBLIC IP(S) ID TYPE REGION ZONE 10.0.7.218 10.0.7.218 34.214.200.181 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S1 agent aws/us-west-2 aws/us-west-2a 10.0.1.127 10.0.1.127 dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 agent aws/us-west-2 aws/us-west-2a master.mesos. 10.0.7.173 34.219.206.248 dedbb786-feb7-47f2-ae69-27bf86ba53fb master (leader) aws/us-west-2 aws/us-west-2a From the output for the dcos node list command, you can see the public IP address that corresponds with the private IP address where the marathon-lb task is running. In this example, the public IP address for the Marathon-LB service is 34.214.200.181.","title":"Install the load balancer"},{"location":"native-app/#connect-using-the-public-ip-address","text":"Connect to the web app from your local computer using the public IP address and port 10000. For example: 34.214.200.181:10000 . You should see a simple web page form similar to this: Add a new Key and add a new Value, then click Save using the sample app web-based frontend. Verify the total number of keys using the app1 sample application by running the dcos task log app1 command. Check Redis directly by running dcos task , copying the Mesos ID returned for the Redis service, then opening a secure shell (SSH) on the node where the Redis service is running. For example, if the dcos task output displays dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 for the Redis task Mesos ID column, you can connect to the node using the dcos node ssh --master-proxy --mesos-id=dedbb786-feb7-47f2-ae69-27bf86ba53fb-S0 command, then so the following: List the Docker containers for the agent using docker ps . Copy the ContainerID for the Redis task from the output for the docker ps command. Create a bash session in the Docker container using the ContainerID from the previous and running sudo docker exec -i -t CONTAINER_ID /bin/bash . Start the Redis CLI by running redis-cli in the bash shell. Check for the key value you added by running the get newkey command.","title":"Connect using the public IP address"},{"location":"native-app/#next-steps","text":"Congratulations! You have deployed a sample application that uses the native DC/OS UCR container, used Marathon-LB to expose the application to the public, and tested your publicly-available app by adding a new key to the Redis service using the web frontend.","title":"Next steps"},{"location":"native-app/#related-topics","text":"DC/OS uses containerizers to run tasks in containers. Running tasks in containers enables you to isolate tasks from each other and control task resources programmatically. DC/OS supports two types of containerizers - the DC/OS Universal Container Runtime, and the Docker containerizer. At this point, you have seen how to deployed apps using a Docker image (app1) and using the native Universal Container Runtime (app2). For your first app, you used a Docker container image to package dependencies so that you didn\u2019t need to rely on particular programs being available on the agent, then used the Docker containerizer to run the app packaged in the Docker image. Because the Docker containerizer internally uses the Docker runtime , you also used the Docker runtime. For your second app, you did not have any dependencies and therefore could rely on the default DC/OS Universal Container Runtime. Internally, both runtimes use the same OS features for isolation, namely cgroups and namespaces .","title":"Related topics"},{"location":"resources/","text":"Allocate and scale resources Provides strategies and examples for scaling resources allocated for deployed applications To this point, you have seen how to create a cluster and how to deploy and test applications and services that run on the cluster. You\u2019ve worked with single commands and apps that run in Docker and DC/OS UCR containers. With this tutorial, you will see some of the key benefits provider by container orchestration and perform a few common resource scaling tasks. Orchestration and cluster management Container orchestration plays a key role in cluster management. Container orchestration helps you manage the lifecycle for apps deployed on the cluster by providing features that address important requirements, such as resilient operation, resource allocation, and service management. Cluster resiliency Container orchestration helps ensure resilient operation by: - Determining the appropriate location for the initial placement of containers based on the current state, size, and configuration of the cluster. Supporting distributed processing and scaling to optimize performance, fault-tolerance, and high availability. Providing self-contained health monitoring and self-healing operations to resurrect failed containers or agents so activity can continue without disruption. Simplifying the deployment of software upgrades or downgrades. Resource allocation and usage Container orchestration improves your ability to manage resources allocation and monitor resource consumption by making sure containers get the specific resources they need to run, including: - Memory CPU Disk GPU Volumes Ports IP addresses Images and artifacts Service identification and management In combination with other features, container orchestration helps you to organize, distribute, and monitor the services you deploy. For example, you can: - Add labels to services to create metadata for querying and organizing services. Use groups or namespaces to define a hierarchy of services and the relationships between them, including dependencies. Monitor health and readiness to ensure applications are available and running properly. This tutorial focuses on resource management and resource isolation between tasks. These two activities are core functions of any operating system and are central elements of effective cluster administration. In this tutorial, you will learn how to monitor resource usage, how resource limits are enforced, and how to debug resource management issues. For information about other aspects of container orchestration, see the Related topics listed at the end of this tutorial. Before you begin Before starting this tutorial, you should verify the following: - You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the sample dcos-101/app2 application deployed and running in your cluster. Learning objectives By completing this tutorial, you will learn: - How to add resource placement constraints for apps. How to monitor and understand your resource utilization. How resource limits are enforced. How to debug resource management issues. Review the app definition If you take another look at the app definition for the app2 sample application, you can see the resources allocated for the app in the cpus , mem , disk , and gpus settings. For example: { id : /dcos-101/app2 , cmd : chmod u+x app2 ./app2 , args : null, user : null, env : null, instances : 1, cpus : 1, mem : 128, disk : 0, gpus : 0, executor : null, constraints : null, Resource allocation before scaling The values for cpus , mem , disk , and gpus define the maximum for each of these resources that a task can use. Tasks rarely use the maximum resources allocated, but these settings specify an upper limit to what you will allow a task to use. Using groups for common resource requirements You might have noticed that the identifier in the app definitions for both app1 and app2 is prefixed by /dcos-101/ . This common identifier is used to define the specific application group that both sample applications belong to. Application groups allow you to specify and apply configuration details and dependencies to multiple applications at the same time. Scale applications When you need more resources for your application, you can scale resources horizontally or vertically . Horizontal scaling involves increasing or decreasing the number of instances of an application. You can scale the instance count in two ways: By setting a factor to apply to an entire application group by a factor. By setting a specific number of instances for an individual application. Scale the application group Since both appl and app2 sample application share the same application group, you can scale them together. To see how you can scale up and scale down an entire application group, do the following: Scale the application group up using a scale factor of two by running the following command: bash dcos marathon group scale dcos-101 2 Verify that both sample apps have scaled up by running the following command: bash dcos marathon app list Scale the application group down again by running the following command: bash dcos marathon group scale dcos-101 0.5 Verify that both apps have scaled down by running the following command: bash dcos marathon app list Set instance count directly In some cases, you might want to scale a single application independently. To see how you can scale up and scale down an entire application group, do the following: Scale app2 to three instances by running the following command: bash dcos marathon app update /dcos-101/app2 instances=3 The instance updates are applied incrementally to an existing app definition. Verify that app2 has scaled by running the following command: bash dcos marathon app list Scale app2 back to one instance by running the following command: bash dcos marathon app update /dcos-101/app2 instances=1 Verify that app2 has scaled by running the following command: bash dcos marathon app list Scale allocated resources Vertical scaling involves increasing or decreasing the resources, such as CPU or memory, that are allocated to an application instance. You should keep in mind that vertical scaling requires restarting the application, which can affect service availability. In a production environment, you should plan for resource scaling and incorporate any changes into your scheduled maintenance periods, if possible. Scale up to two CPUs for the app2 instance by running the following command: bash dcos marathon app update /dcos-101/app2 cpus=2 Verify that app2 has scaled by running the following command: bash dcos marathon app list Scale back down to one CPU for the app2 instance by running the following command: bash dcos marathon app update /dcos-101/app2 cpus=1 Debug resource problems When you are managing resources for the applications running on the DC/OS cluster, there are a few common issues that you should learn how to identify and address. The next topics cover a few of these cases. Not enough resources in the cluster To simulate this issue, try increasing the number of app2 instances by running a command similar to the following: dcos marathon app update /dcos-101/app2 instances=100 This example increases the number of instances to 100. If you have a large cluster, you might need to set the number of instances even higher. Symptom After increasing the number of instances, run dcos marathon app list or dcos marathon deployment list to check that the scale deployment is stuck. Cause The problem here is that there are no matching resources available. For example, there might be resources left for the public agent role, but not for the default role. Solution To resolve this issue, you can add nodes to the cluster or scale the application back to a level at which resources are available. For example, run a command similar to the following: dcos marathon app update /dcos-101/app2 --force instances=1 You must use the --force option in this command because the previous deployment is ongoing. Not enough resources on a single node Because each application is started on a single node, task resources must also fit onto a single node. To simulate this issue, try updating the app2 app to use 100 CPUs by running a command similar to the following: dcos marathon app update /dcos-101/app2 cpus=100 Symptom After increasing the number of CPUs, run dcos marathon app list or dcos marathon deployment list to check that the restart deployment is stuck. Cause The problem here is that there are no resource offers large enough to match the request. Solution To resolve this issue, you can provision larger or scale the application back to a level at which it fits onto the free resources on a single node. For example, run a command similar to the following: dcos marathon app update /dcos-101/app2 --force cpus=1 You must use the --force option in this command because the previous deployment is ongoing. Insufficient resource allocation or resource isolation In some cases, you might have an application that attempts to use more resources than it is allocated. This is a common problem with memory consumption in conjunction with JVM-based applications. To simulate this issue, try deploying the sample out-of-memory app by running the following command: dcos marathon app add https://raw.githubusercontent.com/joerg84/dcos-101/master/oomApp/oomApp.json Symptom After deploying the sample app, check the Marathon log to see if it includes Out of Memory errors. (Because the kernel is killing the app, the errors are not always visible to DC/OS.) Open a terminal and secure shell (SSH) session on an agent where the app run by running a command similar to the following: bash dcos node ssh --master-proxy --mesos-id=$(dcos task oom-app --json | jq -r '.[] | .slave_id')` Check the kernel log by running the following command: bash journalctl -f _TRANSPORT=kernel` The log file should include a message similar to the following: Memory cgroup out of memory: Kill process 10106 (oomApp) score 925 or sacrifice child; Killed process 10390 (oomApp) total-vm:3744760kB, anon-rss:60816kB, file-rss:1240kB, shmem-rss:0kB` Cause In most cases, there are two potential reasons for your application to be using too much memory: There are issues in the application code causing the app to use too much memory, for example, because there is a memory leak in the code logic. You have allocated too little memory for the application. Solution To resolve these potential issues, check the application code to correct any programming errors such. If the problem is not in the code itself, increase the amount of memory you have allocated for the application. To complete this tutorial, be sure to remove the out-of-memory application by running the following command: dcos marathon app remove /dcoc-101/oom-app Next steps In this tutorial, you learned how to view the resources allocated for application tasks and how to scale and debug potential resource issues. Related topics Now that you are practically a pro, you might want to begin exploring more advanced topics and configuration options such as: - Using application groups and labels Defining placement constraints Deploying applications in pods","title":"Allocate and scale resources"},{"location":"resources/#allocate-and-scale-resources","text":"","title":"Allocate and scale resources"},{"location":"resources/#orchestration-and-cluster-management","text":"Container orchestration plays a key role in cluster management. Container orchestration helps you manage the lifecycle for apps deployed on the cluster by providing features that address important requirements, such as resilient operation, resource allocation, and service management.","title":"Orchestration and cluster management"},{"location":"resources/#cluster-resiliency","text":"Container orchestration helps ensure resilient operation by: - Determining the appropriate location for the initial placement of containers based on the current state, size, and configuration of the cluster. Supporting distributed processing and scaling to optimize performance, fault-tolerance, and high availability. Providing self-contained health monitoring and self-healing operations to resurrect failed containers or agents so activity can continue without disruption. Simplifying the deployment of software upgrades or downgrades.","title":"Cluster resiliency"},{"location":"resources/#resource-allocation-and-usage","text":"Container orchestration improves your ability to manage resources allocation and monitor resource consumption by making sure containers get the specific resources they need to run, including: - Memory CPU Disk GPU Volumes Ports IP addresses Images and artifacts","title":"Resource allocation and usage"},{"location":"resources/#service-identification-and-management","text":"In combination with other features, container orchestration helps you to organize, distribute, and monitor the services you deploy. For example, you can: - Add labels to services to create metadata for querying and organizing services. Use groups or namespaces to define a hierarchy of services and the relationships between them, including dependencies. Monitor health and readiness to ensure applications are available and running properly. This tutorial focuses on resource management and resource isolation between tasks. These two activities are core functions of any operating system and are central elements of effective cluster administration. In this tutorial, you will learn how to monitor resource usage, how resource limits are enforced, and how to debug resource management issues. For information about other aspects of container orchestration, see the Related topics listed at the end of this tutorial.","title":"Service identification and management"},{"location":"resources/#before-you-begin","text":"Before starting this tutorial, you should verify the following: - You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the sample dcos-101/app2 application deployed and running in your cluster.","title":"Before you begin"},{"location":"resources/#learning-objectives","text":"By completing this tutorial, you will learn: - How to add resource placement constraints for apps. How to monitor and understand your resource utilization. How resource limits are enforced. How to debug resource management issues.","title":"Learning objectives"},{"location":"resources/#review-the-app-definition","text":"If you take another look at the app definition for the app2 sample application, you can see the resources allocated for the app in the cpus , mem , disk , and gpus settings. For example: { id : /dcos-101/app2 , cmd : chmod u+x app2 ./app2 , args : null, user : null, env : null, instances : 1, cpus : 1, mem : 128, disk : 0, gpus : 0, executor : null, constraints : null,","title":"Review the app definition"},{"location":"resources/#resource-allocation-before-scaling","text":"The values for cpus , mem , disk , and gpus define the maximum for each of these resources that a task can use. Tasks rarely use the maximum resources allocated, but these settings specify an upper limit to what you will allow a task to use.","title":"Resource allocation before scaling"},{"location":"resources/#using-groups-for-common-resource-requirements","text":"You might have noticed that the identifier in the app definitions for both app1 and app2 is prefixed by /dcos-101/ . This common identifier is used to define the specific application group that both sample applications belong to. Application groups allow you to specify and apply configuration details and dependencies to multiple applications at the same time.","title":"Using groups for common resource requirements"},{"location":"resources/#scale-applications","text":"When you need more resources for your application, you can scale resources horizontally or vertically . Horizontal scaling involves increasing or decreasing the number of instances of an application. You can scale the instance count in two ways: By setting a factor to apply to an entire application group by a factor. By setting a specific number of instances for an individual application.","title":"Scale applications"},{"location":"resources/#scale-the-application-group","text":"Since both appl and app2 sample application share the same application group, you can scale them together. To see how you can scale up and scale down an entire application group, do the following: Scale the application group up using a scale factor of two by running the following command: bash dcos marathon group scale dcos-101 2 Verify that both sample apps have scaled up by running the following command: bash dcos marathon app list Scale the application group down again by running the following command: bash dcos marathon group scale dcos-101 0.5 Verify that both apps have scaled down by running the following command: bash dcos marathon app list","title":"Scale the application group"},{"location":"resources/#set-instance-count-directly","text":"In some cases, you might want to scale a single application independently. To see how you can scale up and scale down an entire application group, do the following: Scale app2 to three instances by running the following command: bash dcos marathon app update /dcos-101/app2 instances=3 The instance updates are applied incrementally to an existing app definition. Verify that app2 has scaled by running the following command: bash dcos marathon app list Scale app2 back to one instance by running the following command: bash dcos marathon app update /dcos-101/app2 instances=1 Verify that app2 has scaled by running the following command: bash dcos marathon app list","title":"Set instance count directly"},{"location":"resources/#scale-allocated-resources","text":"Vertical scaling involves increasing or decreasing the resources, such as CPU or memory, that are allocated to an application instance. You should keep in mind that vertical scaling requires restarting the application, which can affect service availability. In a production environment, you should plan for resource scaling and incorporate any changes into your scheduled maintenance periods, if possible. Scale up to two CPUs for the app2 instance by running the following command: bash dcos marathon app update /dcos-101/app2 cpus=2 Verify that app2 has scaled by running the following command: bash dcos marathon app list Scale back down to one CPU for the app2 instance by running the following command: bash dcos marathon app update /dcos-101/app2 cpus=1","title":"Scale allocated resources"},{"location":"resources/#debug-resource-problems","text":"When you are managing resources for the applications running on the DC/OS cluster, there are a few common issues that you should learn how to identify and address. The next topics cover a few of these cases.","title":"Debug resource problems"},{"location":"resources/#not-enough-resources-in-the-cluster","text":"To simulate this issue, try increasing the number of app2 instances by running a command similar to the following: dcos marathon app update /dcos-101/app2 instances=100 This example increases the number of instances to 100. If you have a large cluster, you might need to set the number of instances even higher.","title":"Not enough resources in the cluster"},{"location":"resources/#symptom","text":"After increasing the number of instances, run dcos marathon app list or dcos marathon deployment list to check that the scale deployment is stuck.","title":"Symptom"},{"location":"resources/#cause","text":"The problem here is that there are no matching resources available. For example, there might be resources left for the public agent role, but not for the default role.","title":"Cause"},{"location":"resources/#solution","text":"To resolve this issue, you can add nodes to the cluster or scale the application back to a level at which resources are available. For example, run a command similar to the following: dcos marathon app update /dcos-101/app2 --force instances=1 You must use the --force option in this command because the previous deployment is ongoing.","title":"Solution"},{"location":"resources/#not-enough-resources-on-a-single-node","text":"Because each application is started on a single node, task resources must also fit onto a single node. To simulate this issue, try updating the app2 app to use 100 CPUs by running a command similar to the following: dcos marathon app update /dcos-101/app2 cpus=100","title":"Not enough resources on a single node"},{"location":"resources/#symptom_1","text":"After increasing the number of CPUs, run dcos marathon app list or dcos marathon deployment list to check that the restart deployment is stuck.","title":"Symptom"},{"location":"resources/#cause_1","text":"The problem here is that there are no resource offers large enough to match the request.","title":"Cause"},{"location":"resources/#solution_1","text":"To resolve this issue, you can provision larger or scale the application back to a level at which it fits onto the free resources on a single node. For example, run a command similar to the following: dcos marathon app update /dcos-101/app2 --force cpus=1 You must use the --force option in this command because the previous deployment is ongoing.","title":"Solution"},{"location":"resources/#insufficient-resource-allocation-or-resource-isolation","text":"In some cases, you might have an application that attempts to use more resources than it is allocated. This is a common problem with memory consumption in conjunction with JVM-based applications. To simulate this issue, try deploying the sample out-of-memory app by running the following command: dcos marathon app add https://raw.githubusercontent.com/joerg84/dcos-101/master/oomApp/oomApp.json","title":"Insufficient resource allocation or resource isolation"},{"location":"resources/#symptom_2","text":"After deploying the sample app, check the Marathon log to see if it includes Out of Memory errors. (Because the kernel is killing the app, the errors are not always visible to DC/OS.) Open a terminal and secure shell (SSH) session on an agent where the app run by running a command similar to the following: bash dcos node ssh --master-proxy --mesos-id=$(dcos task oom-app --json | jq -r '.[] | .slave_id')` Check the kernel log by running the following command: bash journalctl -f _TRANSPORT=kernel` The log file should include a message similar to the following: Memory cgroup out of memory: Kill process 10106 (oomApp) score 925 or sacrifice child; Killed process 10390 (oomApp) total-vm:3744760kB, anon-rss:60816kB, file-rss:1240kB, shmem-rss:0kB`","title":"Symptom"},{"location":"resources/#cause_2","text":"In most cases, there are two potential reasons for your application to be using too much memory: There are issues in the application code causing the app to use too much memory, for example, because there is a memory leak in the code logic. You have allocated too little memory for the application.","title":"Cause"},{"location":"resources/#solution_2","text":"To resolve these potential issues, check the application code to correct any programming errors such. If the problem is not in the code itself, increase the amount of memory you have allocated for the application. To complete this tutorial, be sure to remove the out-of-memory application by running the following command: dcos marathon app remove /dcoc-101/oom-app","title":"Solution"},{"location":"resources/#next-steps","text":"In this tutorial, you learned how to view the resources allocated for application tasks and how to scale and debug potential resource issues.","title":"Next steps"},{"location":"resources/#related-topics","text":"Now that you are practically a pro, you might want to begin exploring more advanced topics and configuration options such as: - Using application groups and labels Defining placement constraints Deploying applications in pods","title":"Related topics"},{"location":"schedule-jobs/","text":"Create and schedule jobs Provides instructions for creating, scheduling, and managing jobs to run on a DC/OS cluster You can create and administer jobs for the DC/OS cluster in any of the following ways: - interactively with the DC/OS web-based administrative console GUI. - interactively or programmatically with DC/OS command-line interface (CLI) programs. - directly through calls to the DC/OS application programming interface (API) for job-related operations. The DC/OS application programming interface (API) for job-related operations provides the underlying functionality that you can access through the DC/OS web-based administrative console and command-line interface (CLI). In most cases, therefore, you would only use the API directly if you are integrating the functionality with a custom program or automation script. Manage jobs with the DC/OS web-based interface You can manage the most common job-related activity interactively through the DC/OS web-based interface. For example, you can add, modify, run, and remove jobs directly from the Jobs tab in the web-based console. However, the DC/OS web-based interface only provides access to a subset of the job-related functionality provided through the dcos job CLI and Jobs API. For more advanced job configurations and activity, use the dcos job commands or the Jobs API . Add a job Navigate to the URL for the DC/OS web-based console in a web browser. Click Jobs , then click Create a Job to display the New Job settings. You can configure the job using the fields displayed or click JSON Editor to edit the JSON directly. If you click Jobs and see a list of previously-created jobs, click the plus (+) sign displayed above and to the right of the job list to create a new job. Click General to edit the most basic job settings, such as the job identifier, CPU, memory, and disk requirements. Job ID - Defines a unique identifier for the new job. The Job ID is a required field. You also use this setting to manage job operations. Description - Provides an optional description of the new job. CPUs - Specifies the number of CPU cores your job requires. This field is required for all jobs. Mem - Specifies the amount of memory, in MB, your job requires. This field is required for all jobs. Disk - Specifies the amount of disk space, in MB, your job requires. This field is required for all jobs. GPUs - Specifies the number of GPU (Graphics Processing Unit) cores to allocate for processing your job. This field is only applicable for jobs that run on nodes configured to use GPU (Graphics Processing Unit) cores. Select the appropriate Job Type to run one or more specific commands or a Docker container image. Select Command Only to specify one or more commands you want the new job to execute. If you select Command Only , you must specify the command or command arguments to execute. When the command you specify is executed, it is automatically wrapped by the command /bin/sh -c job.cmd . You must include either cmd or args in the command to be executed. It is invalid to supply both cmd and args in the same job. If you select the Command Only option, none of the Container Runtime settings are applicable for the job. You can continue creating the job by defining Schedule settings, adding advanced Run Configuration options, or clicking Submit . Select Container Image to specify a container image for the new job. If you select this option, type the name of the container image you want to run. For example, you can type a container image name such as ubuntu:14.04 . You can then use the Command field to specify the command and any additional runtime parameters available in the container for running the new job. If you select the Container Image option, you can continue creating the job by: - Configuring Container Runtime settings for the job. - Defining a job Schedule , if applicable. - Adding advanced Run Configuration options, if applicable. - Clicking Submit . Click Container Runtime to specify whether the container for the new job runs using the Universal Container Runtime or the Docker engine. If you select Universal Container Runtime , you can select Force Pull Image On Launch to automatically pull the latest image before launching each instance. If you select Docker Engine , you can select the following additional options: Select Force Pull Image On Launch to automatically pull the latest image before launching each instance. Select Grant Runtime Privileges to run the specified Docker image in privileged mode. Click Add Parameter to specify additional Docker runtime parameter names and values for the new job, if applicable. You can add multiple parameter names and corresponding values by clicking Add Parameter for each parameter name and value you want to include. Click Add Arg to specify additional command-line arguments for the new job, if applicable. You can add multiple arguments by clicking Add Arg for each argument you want to include. Click Schedule , then click Enable Schedule to specify a schedule for when the job runs. Select Enable Schedule if you want to run the job using the schedule you define with the following settings: Type a Schedule ID to define a unique identifier for the job schedule. The schedule identifier must be a string with at least 2 characters and it can only contain digits ( 0-9 ), dashes ( - ), and lowercase letters ( a-z ). The schedule identifier must not begin or end with a dash. Select CRON Schedule to specify the schedule in cron format. Use this crontab generator for help. You can also set a Time Zone to apply to the cron schedule. For example, you might have nodes in different time zones and want to run the job using a standardized UTC time or a specific local time zone such as America/New_York. Select Starting Deadline to set the time, in seconds, to start the job if it misses its scheduled time for any reason. Missed job executions are counted as failed jobs. Select Concurrency Policy if you want to allow new job instances to run if there's already a job instance running. After you define the schedule, you can activate or deactivate it by selecting or deselecting the Enable Schedule option. You can also modify or remove the schedule when needed after you have submitted the new job definition. Click Run Configuration to specify advanced settings for the new job. Set Max Launch Delay to specify the maximum number of seconds to wait for a job to start running after it is launched by a scheduled job run or manually by a user. If the job does not start running within the maximum number of seconds allowed, the job is aborted. Set Kill Grace Period to configure the number of seconds between escalating from SIGTERM to SIGKILL when signalling tasks to terminate. During this grace period, tasks should perform an orderly shut down immediately upon receiving SIGTERM. Set User name to identify the user account that runs the tasks on the agent. Select Add Artifact to provide one or more artifact URIs you want passed to fetcher module and resolved at runtime and the action--Execute, Extract, or Cache--you want to perform for each URI. Select a Restart Policy to determine the steps to take if a job fails. You can choose Never if you never want to attempt to restart a failed job. If you choose On Failure , you can set a time limit for attempting to restart the job using the Keep Trying Time field. For example, set the Keep Trying Time to 30 if you want to try restarting the job after waiting for 30 seconds. If no value is set for Keep Trying Time, DC/OS will continue attempting to restart the failed job indefinitely. Click Add Label to specify a Key and Value that you want to attach as metadata to the new job. You can then use the job label to filter or expose information for labeled jobs. You can add multiple label key name/value pairs by clicking Add Label for each name/value pair you want to include. For more information about using labels, see Labeling tasks and jobs . Click Submit to create the job. Verify that you have added the new job by clicking Jobs . Add a job to a job group You can add a job to a an existing job group or create a new job group when you create the job. Use dots in your job ID to nest the job in a group. For example, if you add a job using the job ID marketing.myjob , the new myjob is created in the marketing job group. In DC/OS Enterprise, you can use job groups to implement fine-grained user access. For more information about controlling access to jobs through job groups, see Granting access to jobs . View, modify, or remove a specific job You can view and modify job-related information, including details about the run history and configuration settings interactively through the DC/OS web-based interface. From the Jobs tab, click the name of your job. You can then use the menu on the upper right to edit, run, disable, or delete a selected job. While the job is running, you can click the job instance to drill down to Details , Files , and Logs data. Managing jobs with the DC/OS CLI You can create and manage jobs from the DC/OS CLI using dcos job commands. To see a full list of available commands with usage information, run dcos job --help . Create a JSON file for a new job Open a new file in a text editor to create a job file in JSON format. In the new file, specify the basic parameters required to define the job, including the following: the job id you use to manage the job the specific command to run the CPU, memory, and disk requirements the job schedule For example, the JSON file for a new job might look similar to this: json { \"id\": \"myjob\", \"description\": \"A job that sleeps regularly\", \"run\": { \"cmd\": \"sleep 20000\", \"cpus\": 0.01, \"mem\": 32, \"disk\": 0 }, \"schedules\": [ { \"id\": \"sleep-schedule\", \"enabled\": true, \"cron\": \"20 0 * * *\", \"concurrencyPolicy\": \"ALLOW\" } ] } Save the JSON file for the new job using an easily-identifiable file name. For example, you might save the job information as mysleepjob.json . Add the job by running a command similar to the following: bash dcos job add myjob .json For example: bash dcos job add mysleepjob.json 1. Verify that you have added the new job by running a command similar to the following: bash dcos job list The command displays the list of jobs similar to the following: bash ID STATUS LAST RUN mysleepjob Scheduled N/A mypingjob Running N/A Set a concurrency policy for scheduled jobs If you use a schedule to start a job, you can define a concurrency policy for the job. A concurrency policy determines whether a new job run instance is triggered if there's already a job instance running. For example, assume you have a job scheduled to start every day at 3:00AM, and you have set the concurrency policy for the job set to FORBID. If there is an instance of that job already running at 3:00AM--either because a previously-triggered job run is still active or has been triggered manually outside of the schedule--the scheduled start time will not trigger a new job to run. If there are no jobs running at the next scheduled start time, a new job instance starts and runs as scheduled. If you want to allow scheduled jobs to be triggered while other instances of the same job are running, you can set the concurrencyPolicy to ALLOW. Create a schedule-only JSON file If you specify a schedule for a job in the JSON file for that job, you can assign only one schedule for the job to run under. If you want to use the same schedule for more than one job, however, you can create a separate JSON file specifically for the schedule. You can then use the dcos job schedule add job-id schedule-file command to associate a job with the schedule. Open a file in a text editor to create a new job file in JSON format, if necessary. You must use the job id you define for the job to associate a schedule JSON file with the job. To prevent schedule conflicts or unexpected job runs, you should not define schedule parameters for a job if you want to use the schedule-only JSON file to control when a job runs. Open a new file in a text editor to create the schedule you want to use in JSON format. For example, the JSON file for a new schedule might look similar to this: json { \"concurrencyPolicy\": \"ALLOW\", \"cron\": \"20 0 * * *\", \"enabled\": true, \"id\": \"nightly\", \"nextRunAt\": \"2016-07-26T00:20:00.000+0000\", \"startingDeadlineSeconds\": 900, \"timezone\": \"UTC\" } Save the JSON file for the new schedule using an easily-identifiable file name. For example, you might save the schedule information as my-cron-def.json . Associate the job with the schedule by running a command similar to the following: bash dcos job schedule add job-id schedule-file For example: bash dcos job schedule add mytestjob my-cron-def.json If you attempt to add a schedule definition to a job that already has a schedule defined, the command displays an error similar to the following: Error: 409 - requirement failed: A schedule with id nightly already exists Verify that you have added the new job schedule by running a command similar to the following: bash dcos job schedule show mytestjob This command displays schedule information for the specified job similar to the following: bash ID CRON ENABLED NEXT RUN CONCURRENCY POLICY nightly 20 0 * * * True 2019-04-11T00:20:00.000+0000 ALLOW Start a job from the command line You can trigger a job to run: - manually on-demand - automatically based on a schedule you have defined - programmatically through automation with or without a schedule You can use any of these approaches to start a job instance that is referred to as a job run. For example, you can use the DC/OS command-line interface to start jobs regardless of whether you have defined a schedule or not. Starting a job manually from the command-line is similar to starting a job by clicking Run now using the DC/OS web-based console. To start a job run on-demand outside of any scheduled job activity, run a command similar to the following: dcos job run job-id For example, if the job id is mytestjob , run: dcos job run mytestjob Starting a job manually from the command-line or through the DC/OS web-based console triggers a new job run each time the command is executed. Jobs that are triggered manually on-demand ignore concurrency policy settings. If a schedule is used to start a job, however, the job's concurrency policy determines whether a new job run instance is triggered. Being able to control whether jobs run concurrently is one of the main differences between triggering a job to run manually or using a schedule. Remove a job from the command-line You can remove a job using the command-line program dcos job remove as long as the job does not have any active job instances running. If a job has any currently active running instances, you must stop all of the currently-active jobs. After you stop all running job instances, you can remove the job using the dcos job remove job-id command. To remove a job: 1. Check the status of active jobs by running a command similar to the following: ```bash dcos job list ``` Stop all running job instances for the job you want to delete and remove the job by running the following command: bash dcos job remove job-id --stop-current-job-runs Verify that you have removed the specified job by running the following command: dcos job list Modify a job from the command line To modify your job, update your JSON job file, then run dcos job update job-file .json Modify a job's schedule You can update the schedule of your job in two ways, depending if your job has a schedule specified in the job-file .json or if your job's schedule is kept in a separate file. Modify a job with a schedule Modify the schedules parameter of your job-file .json . Then run dcos job update job-file .json Modify a job with a separate schedule file Modify schedule-file .json . Then, run one of the following commands: dcos job schedule add job-id schedule-file .json dcos job schedule remove job-id schedule-id dcos job schedule update job-id schedule-file .json View job details List all jobs: dcos job list List all previous runs of your job: dcos job history job-id To view details about your job, run: dcos job show job-id To view details about your job's schedule, run: dcos job schedule show job-id View job logs To view the log for your job: dcos task log --completed job-id To get the log for only a specific job run, use a job run ID from dcos job history job-id dcos task log --completed job-run-id Using the Jobs API You can also create and administer jobs through calls to the Jobs API endpoints. This section highlights the most common tasks you perform through job-related API calls. For more complete information about the Jobs API, see the Jobs API reference information. Preparing to use API cals The code examples in this section illustrate how to include Jobs API calls to perform job-related tasks with the client URL (cURL) program. For detailed information about using curl command, see the curl man page . In addition, one important difference between using the DC/OS command-line interface or web-based console and the API is how you configure the job schedule. The DC/OS CLI and web-based console support a combined JSON format that allows you to specify a schedule in the job descriptor. To schedule a job using the Jobs API, you must use two separate calls: - Use one call to add an unscheduled job. - Use a second call to associate a specific schedule file ( schedule-file.json ) with the job. Add a job using an API call The following command adds a job called myjob.json . curl -X POST -H Content-Type: application/json -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs -d@/Users/ your-username / myjob .json Remove a job using an API call The following command removes a job regardless of whether the job is running: curl -X DELETE -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ myjob ?stopCurrentJobRuns=true To remove a job only if it is not running, set stopCurrentJobRuns to False . Modify or view a job using an API call The following command shows all jobs: curl -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs The following command lists job runs: curl -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ myjob /runs/ Stop a run with the following command: curl -X POST -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ myjob /runs/20160725212507ghwfZ/actions/stop Add a schedule to a job The following command adds a schedule to a job: curl -X POST -H Content-Type: application/json -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ job-id /schedules -d@ schedule-file .json Start a job using an API call You can use the DC/OS API to start jobs programmatically. Similar to starting a job using the web-based console or command-line interface, you must specify the job identifer in the call. To trigger a job run to start you can use a REST API call similar to the following: curl -X POST -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/{jobId}/runs","title":"Create and schedule jobs"},{"location":"schedule-jobs/#create-and-schedule-jobs","text":"","title":"Create and schedule jobs"},{"location":"schedule-jobs/#manage-jobs-with-the-dcos-web-based-interface","text":"You can manage the most common job-related activity interactively through the DC/OS web-based interface. For example, you can add, modify, run, and remove jobs directly from the Jobs tab in the web-based console. However, the DC/OS web-based interface only provides access to a subset of the job-related functionality provided through the dcos job CLI and Jobs API. For more advanced job configurations and activity, use the dcos job commands or the Jobs API .","title":"Manage jobs with the DC/OS web-based interface"},{"location":"schedule-jobs/#add-a-job","text":"Navigate to the URL for the DC/OS web-based console in a web browser. Click Jobs , then click Create a Job to display the New Job settings. You can configure the job using the fields displayed or click JSON Editor to edit the JSON directly. If you click Jobs and see a list of previously-created jobs, click the plus (+) sign displayed above and to the right of the job list to create a new job. Click General to edit the most basic job settings, such as the job identifier, CPU, memory, and disk requirements. Job ID - Defines a unique identifier for the new job. The Job ID is a required field. You also use this setting to manage job operations. Description - Provides an optional description of the new job. CPUs - Specifies the number of CPU cores your job requires. This field is required for all jobs. Mem - Specifies the amount of memory, in MB, your job requires. This field is required for all jobs. Disk - Specifies the amount of disk space, in MB, your job requires. This field is required for all jobs. GPUs - Specifies the number of GPU (Graphics Processing Unit) cores to allocate for processing your job. This field is only applicable for jobs that run on nodes configured to use GPU (Graphics Processing Unit) cores. Select the appropriate Job Type to run one or more specific commands or a Docker container image. Select Command Only to specify one or more commands you want the new job to execute. If you select Command Only , you must specify the command or command arguments to execute. When the command you specify is executed, it is automatically wrapped by the command /bin/sh -c job.cmd . You must include either cmd or args in the command to be executed. It is invalid to supply both cmd and args in the same job. If you select the Command Only option, none of the Container Runtime settings are applicable for the job. You can continue creating the job by defining Schedule settings, adding advanced Run Configuration options, or clicking Submit . Select Container Image to specify a container image for the new job. If you select this option, type the name of the container image you want to run. For example, you can type a container image name such as ubuntu:14.04 . You can then use the Command field to specify the command and any additional runtime parameters available in the container for running the new job. If you select the Container Image option, you can continue creating the job by: - Configuring Container Runtime settings for the job. - Defining a job Schedule , if applicable. - Adding advanced Run Configuration options, if applicable. - Clicking Submit . Click Container Runtime to specify whether the container for the new job runs using the Universal Container Runtime or the Docker engine. If you select Universal Container Runtime , you can select Force Pull Image On Launch to automatically pull the latest image before launching each instance. If you select Docker Engine , you can select the following additional options: Select Force Pull Image On Launch to automatically pull the latest image before launching each instance. Select Grant Runtime Privileges to run the specified Docker image in privileged mode. Click Add Parameter to specify additional Docker runtime parameter names and values for the new job, if applicable. You can add multiple parameter names and corresponding values by clicking Add Parameter for each parameter name and value you want to include. Click Add Arg to specify additional command-line arguments for the new job, if applicable. You can add multiple arguments by clicking Add Arg for each argument you want to include. Click Schedule , then click Enable Schedule to specify a schedule for when the job runs. Select Enable Schedule if you want to run the job using the schedule you define with the following settings: Type a Schedule ID to define a unique identifier for the job schedule. The schedule identifier must be a string with at least 2 characters and it can only contain digits ( 0-9 ), dashes ( - ), and lowercase letters ( a-z ). The schedule identifier must not begin or end with a dash. Select CRON Schedule to specify the schedule in cron format. Use this crontab generator for help. You can also set a Time Zone to apply to the cron schedule. For example, you might have nodes in different time zones and want to run the job using a standardized UTC time or a specific local time zone such as America/New_York. Select Starting Deadline to set the time, in seconds, to start the job if it misses its scheduled time for any reason. Missed job executions are counted as failed jobs. Select Concurrency Policy if you want to allow new job instances to run if there's already a job instance running. After you define the schedule, you can activate or deactivate it by selecting or deselecting the Enable Schedule option. You can also modify or remove the schedule when needed after you have submitted the new job definition. Click Run Configuration to specify advanced settings for the new job. Set Max Launch Delay to specify the maximum number of seconds to wait for a job to start running after it is launched by a scheduled job run or manually by a user. If the job does not start running within the maximum number of seconds allowed, the job is aborted. Set Kill Grace Period to configure the number of seconds between escalating from SIGTERM to SIGKILL when signalling tasks to terminate. During this grace period, tasks should perform an orderly shut down immediately upon receiving SIGTERM. Set User name to identify the user account that runs the tasks on the agent. Select Add Artifact to provide one or more artifact URIs you want passed to fetcher module and resolved at runtime and the action--Execute, Extract, or Cache--you want to perform for each URI. Select a Restart Policy to determine the steps to take if a job fails. You can choose Never if you never want to attempt to restart a failed job. If you choose On Failure , you can set a time limit for attempting to restart the job using the Keep Trying Time field. For example, set the Keep Trying Time to 30 if you want to try restarting the job after waiting for 30 seconds. If no value is set for Keep Trying Time, DC/OS will continue attempting to restart the failed job indefinitely. Click Add Label to specify a Key and Value that you want to attach as metadata to the new job. You can then use the job label to filter or expose information for labeled jobs. You can add multiple label key name/value pairs by clicking Add Label for each name/value pair you want to include. For more information about using labels, see Labeling tasks and jobs . Click Submit to create the job. Verify that you have added the new job by clicking Jobs .","title":"Add a job"},{"location":"schedule-jobs/#add-a-job-to-a-job-group","text":"You can add a job to a an existing job group or create a new job group when you create the job. Use dots in your job ID to nest the job in a group. For example, if you add a job using the job ID marketing.myjob , the new myjob is created in the marketing job group. In DC/OS Enterprise, you can use job groups to implement fine-grained user access. For more information about controlling access to jobs through job groups, see Granting access to jobs .","title":"Add a job to a job group"},{"location":"schedule-jobs/#view-modify-or-remove-a-specific-job","text":"You can view and modify job-related information, including details about the run history and configuration settings interactively through the DC/OS web-based interface. From the Jobs tab, click the name of your job. You can then use the menu on the upper right to edit, run, disable, or delete a selected job. While the job is running, you can click the job instance to drill down to Details , Files , and Logs data.","title":"View, modify, or remove a specific job"},{"location":"schedule-jobs/#managing-jobs-with-the-dcos-cli","text":"You can create and manage jobs from the DC/OS CLI using dcos job commands. To see a full list of available commands with usage information, run dcos job --help .","title":"Managing jobs with the DC/OS CLI"},{"location":"schedule-jobs/#create-a-json-file-for-a-new-job","text":"Open a new file in a text editor to create a job file in JSON format. In the new file, specify the basic parameters required to define the job, including the following: the job id you use to manage the job the specific command to run the CPU, memory, and disk requirements the job schedule For example, the JSON file for a new job might look similar to this: json { \"id\": \"myjob\", \"description\": \"A job that sleeps regularly\", \"run\": { \"cmd\": \"sleep 20000\", \"cpus\": 0.01, \"mem\": 32, \"disk\": 0 }, \"schedules\": [ { \"id\": \"sleep-schedule\", \"enabled\": true, \"cron\": \"20 0 * * *\", \"concurrencyPolicy\": \"ALLOW\" } ] } Save the JSON file for the new job using an easily-identifiable file name. For example, you might save the job information as mysleepjob.json . Add the job by running a command similar to the following: bash dcos job add myjob .json For example: bash dcos job add mysleepjob.json 1. Verify that you have added the new job by running a command similar to the following: bash dcos job list The command displays the list of jobs similar to the following: bash ID STATUS LAST RUN mysleepjob Scheduled N/A mypingjob Running N/A","title":"Create a JSON file for a new job"},{"location":"schedule-jobs/#set-a-concurrency-policy-for-scheduled-jobs","text":"If you use a schedule to start a job, you can define a concurrency policy for the job. A concurrency policy determines whether a new job run instance is triggered if there's already a job instance running. For example, assume you have a job scheduled to start every day at 3:00AM, and you have set the concurrency policy for the job set to FORBID. If there is an instance of that job already running at 3:00AM--either because a previously-triggered job run is still active or has been triggered manually outside of the schedule--the scheduled start time will not trigger a new job to run. If there are no jobs running at the next scheduled start time, a new job instance starts and runs as scheduled. If you want to allow scheduled jobs to be triggered while other instances of the same job are running, you can set the concurrencyPolicy to ALLOW.","title":"Set a concurrency policy for scheduled jobs"},{"location":"schedule-jobs/#create-a-schedule-only-json-file","text":"If you specify a schedule for a job in the JSON file for that job, you can assign only one schedule for the job to run under. If you want to use the same schedule for more than one job, however, you can create a separate JSON file specifically for the schedule. You can then use the dcos job schedule add job-id schedule-file command to associate a job with the schedule. Open a file in a text editor to create a new job file in JSON format, if necessary. You must use the job id you define for the job to associate a schedule JSON file with the job. To prevent schedule conflicts or unexpected job runs, you should not define schedule parameters for a job if you want to use the schedule-only JSON file to control when a job runs. Open a new file in a text editor to create the schedule you want to use in JSON format. For example, the JSON file for a new schedule might look similar to this: json { \"concurrencyPolicy\": \"ALLOW\", \"cron\": \"20 0 * * *\", \"enabled\": true, \"id\": \"nightly\", \"nextRunAt\": \"2016-07-26T00:20:00.000+0000\", \"startingDeadlineSeconds\": 900, \"timezone\": \"UTC\" } Save the JSON file for the new schedule using an easily-identifiable file name. For example, you might save the schedule information as my-cron-def.json . Associate the job with the schedule by running a command similar to the following: bash dcos job schedule add job-id schedule-file For example: bash dcos job schedule add mytestjob my-cron-def.json If you attempt to add a schedule definition to a job that already has a schedule defined, the command displays an error similar to the following: Error: 409 - requirement failed: A schedule with id nightly already exists Verify that you have added the new job schedule by running a command similar to the following: bash dcos job schedule show mytestjob This command displays schedule information for the specified job similar to the following: bash ID CRON ENABLED NEXT RUN CONCURRENCY POLICY nightly 20 0 * * * True 2019-04-11T00:20:00.000+0000 ALLOW","title":"Create a schedule-only JSON file"},{"location":"schedule-jobs/#start-a-job-from-the-command-line","text":"You can trigger a job to run: - manually on-demand - automatically based on a schedule you have defined - programmatically through automation with or without a schedule You can use any of these approaches to start a job instance that is referred to as a job run. For example, you can use the DC/OS command-line interface to start jobs regardless of whether you have defined a schedule or not. Starting a job manually from the command-line is similar to starting a job by clicking Run now using the DC/OS web-based console. To start a job run on-demand outside of any scheduled job activity, run a command similar to the following: dcos job run job-id For example, if the job id is mytestjob , run: dcos job run mytestjob Starting a job manually from the command-line or through the DC/OS web-based console triggers a new job run each time the command is executed. Jobs that are triggered manually on-demand ignore concurrency policy settings. If a schedule is used to start a job, however, the job's concurrency policy determines whether a new job run instance is triggered. Being able to control whether jobs run concurrently is one of the main differences between triggering a job to run manually or using a schedule.","title":"Start a job from the command line"},{"location":"schedule-jobs/#remove-a-job-from-the-command-line","text":"You can remove a job using the command-line program dcos job remove as long as the job does not have any active job instances running. If a job has any currently active running instances, you must stop all of the currently-active jobs. After you stop all running job instances, you can remove the job using the dcos job remove job-id command. To remove a job: 1. Check the status of active jobs by running a command similar to the following: ```bash dcos job list ``` Stop all running job instances for the job you want to delete and remove the job by running the following command: bash dcos job remove job-id --stop-current-job-runs Verify that you have removed the specified job by running the following command: dcos job list","title":"Remove a job from the command-line"},{"location":"schedule-jobs/#modify-a-job-from-the-command-line","text":"To modify your job, update your JSON job file, then run dcos job update job-file .json","title":"Modify a job from the command line"},{"location":"schedule-jobs/#modify-a-jobs-schedule","text":"You can update the schedule of your job in two ways, depending if your job has a schedule specified in the job-file .json or if your job's schedule is kept in a separate file.","title":"Modify a job's schedule"},{"location":"schedule-jobs/#modify-a-job-with-a-schedule","text":"Modify the schedules parameter of your job-file .json . Then run dcos job update job-file .json","title":"Modify a job with a schedule"},{"location":"schedule-jobs/#modify-a-job-with-a-separate-schedule-file","text":"Modify schedule-file .json . Then, run one of the following commands: dcos job schedule add job-id schedule-file .json dcos job schedule remove job-id schedule-id dcos job schedule update job-id schedule-file .json","title":"Modify a job with a separate schedule file"},{"location":"schedule-jobs/#view-job-details","text":"List all jobs: dcos job list List all previous runs of your job: dcos job history job-id To view details about your job, run: dcos job show job-id To view details about your job's schedule, run: dcos job schedule show job-id","title":"View job details"},{"location":"schedule-jobs/#view-job-logs","text":"To view the log for your job: dcos task log --completed job-id To get the log for only a specific job run, use a job run ID from dcos job history job-id dcos task log --completed job-run-id","title":"View job logs"},{"location":"schedule-jobs/#preparing-to-use-api-cals","text":"The code examples in this section illustrate how to include Jobs API calls to perform job-related tasks with the client URL (cURL) program. For detailed information about using curl command, see the curl man page . In addition, one important difference between using the DC/OS command-line interface or web-based console and the API is how you configure the job schedule. The DC/OS CLI and web-based console support a combined JSON format that allows you to specify a schedule in the job descriptor. To schedule a job using the Jobs API, you must use two separate calls: - Use one call to add an unscheduled job. - Use a second call to associate a specific schedule file ( schedule-file.json ) with the job.","title":"Preparing to use API cals"},{"location":"schedule-jobs/#add-a-job-using-an-api-call","text":"The following command adds a job called myjob.json . curl -X POST -H Content-Type: application/json -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs -d@/Users/ your-username / myjob .json","title":"Add a job using an API call"},{"location":"schedule-jobs/#remove-a-job-using-an-api-call","text":"The following command removes a job regardless of whether the job is running: curl -X DELETE -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ myjob ?stopCurrentJobRuns=true To remove a job only if it is not running, set stopCurrentJobRuns to False .","title":"Remove a job using an API call"},{"location":"schedule-jobs/#modify-or-view-a-job-using-an-api-call","text":"The following command shows all jobs: curl -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs The following command lists job runs: curl -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ myjob /runs/ Stop a run with the following command: curl -X POST -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ myjob /runs/20160725212507ghwfZ/actions/stop","title":"Modify or view a job using an API call"},{"location":"schedule-jobs/#add-a-schedule-to-a-job","text":"The following command adds a schedule to a job: curl -X POST -H Content-Type: application/json -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/ job-id /schedules -d@ schedule-file .json","title":"Add a schedule to a job"},{"location":"schedule-jobs/#start-a-job-using-an-api-call","text":"You can use the DC/OS API to start jobs programmatically. Similar to starting a job using the web-based console or command-line interface, you must specify the job identifer in the call. To trigger a job run to start you can use a REST API call similar to the following: curl -X POST -H Authorization: token=$(dcos config show core.dcos_acs_token) $(dcos config show core.dcos_url)/service/metronome/v1/jobs/{jobId}/runs","title":"Start a job using an API call"},{"location":"service-discovery/","text":"Discover deployed services Demonstrates how to discover and connect to services in your DC/OS cluster In the previous tutorial, you deployed a sample app that connected to the Redis service. If you reviewed the script for that app, you might have noticed these lines for connecting to the Redis service: print( Running on node ' + os.getenv( HOST ) + ' and port ' + os.getenv( PORT0 )) r = redis.StrictRedis(host='redis.marathon.l4lb.thisdcos.directory', port=6379, db=0) if r.ping(): print( Redis Connected. Total number of keys: , len(r.keys())) else: print( Could not connect to redis ) In this excerpt from the sample app script, you can see that the app uses redis.marathon.l4lb.thisdcos.directory as the service address and port 6379 as the service port. It is important to note, however, that one of the advantages of a distributed computing environment is that a service--like the Redis service in this example--might be running on any agent in the cluster. In addition, the host address and the port number for accessing the service can change dynamically in response to changes in the cluster such as a failed node. This tutorial explores how DC/OS determines the IP addresses and ports where specific service instances are running. Before you begin Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the sample dcos-101/app1 application deployed and running in your cluster. You have the domain information command-line utility dig available on the computer you use to connect to the cluster. The dig utility is part of the DNS BIND utilities that are installed by default with most Linux distributions. Learning objectives By completing this tutorial, you will learn: The service discovery options available in DC/OS. How DC/OS resolves service addresses to find running instances. Select a service discovery method In a DC/OS cluster, service discovery provides a method for finding applications regardless of where they might be running in the cluster. With service discovery, you can find where deployed services are running in the DC/OS cluster in one of two ways: By resolving private or public agent node IP addresses for tasks through the Mesos domain naming service (Mesos-DNS) . Through manually-set named virtual IP addresses that are not resolved through the Mesos domain naming service. Keeping service discovery separate from traditional DNS name resolution (in which service records are associated with a specific physical or virtual IP addresses) is particularly useful in cases where applications fail and might be restarted on a different host. Using Mesos-DNS The most common service discovery option is Mesos-DNS. Mesos-DNS provides a relatively simple method for finding applications inside the cluster. Mesos-DNS assigns DNS entries for every task. These task-specific DNS entries are then resolvable from any node in the cluster. The naming pattern for Mesos-DNS entries is task.scheduler.mesos Because the default scheduler for the Redis service you have deployed is Marathon , the Mesos-DNS name for your Redis service is redis.marathon.mesos or redis-tutorial.marathon.mesos. You can use DNS query tools, such as dig , to retrieve address (A) and service (SRV) record from the DNS nameserver, which in a DC/OS cluster is configured to point at Mesos-DNS. For example, you can run a command similar to this to find the IP address (A) record where an instance of the redis.marathon.mesos service is running: dig redis.marathon.mesos This command returns an answer section be similar to the following indicating that the service is running on the host with the IP address 10.0.0.43: ;; ANSWER SECTION: redis.marathon.mesos. 60 IN A 10.0.0.43 To connect to the service, you also need to know the port. In retrieve this information, Mesos-DNS also assigns each Marathon app a service (SRV) record containing the port number. You can run a command similar to the following to find the Redis SRV record: dig srv _redis._tcp.marathon.mesos This command returns an answer section be similar to the following indicating that the Redis service is running on 10.0.0.43:30585 ``` ;; ANSWER SECTION: _redis._tcp.marathon.mesos. 60 IN SRV 0 0 30585 redis-1y1hj-s1.marathon.mesos. ;; ADDITIONAL SECTION: redis-1y1hj-s1.marathon.mesos. 60 IN A 10.0.0.43 ``` Using Mesos-DNS for service discovery is appropriate for many applications, but has the following drawbacks: Applications sometimes cache DNS entries for efficiency and therefore might not provide updated address information in some cases, such as after a task failure. You need to use SRV DNS records to retrieve information about the allocated ports. While applications commonly understand DNS A records, not all applications support SRV records. Using named virtual IP addresses Named virtual IP addresses (VIPs) enable you to assign name/port pairs to your apps. With this type of service discovery, you can give apps recognizable names with predictable port information. Virtual IP addresses also allow you to take advantage of DC/OS layer-4 load balancing when there are multiple instances of an application. For example, you can assign a named virtual IP address to the Redis service by adding the following to the app definition for the package: \"VIP_0\": \"/redis:6379\" The full name is then generated using the following format: vip-name.scheduler.l4lb.thisdcos.directory:vip-port . This is the discovery method used in the sample application for access the Redis service from within the cluster at redis.marathon.l4lb.thisdcos.directory:6379 . Named VIPs load balance the IP address/port pair using an intelligent algorithm to ensure optimal routing of the traffic in relation to the original requestor, and also provide a local caching layer for high performance. They also allow you to give your apps meaningful names and select a specific port. Because of these advantages over Mesos-DNS,in most cases you should use named virtual IP addresses for service discovery in DC/OS. Next steps You know how to use service discovery to connect to your application from within your DC/OS cluster, and have learned about the two mechanisms for service discovery available in DC/OS. Related topics Now that you have a basic introduction to Mesos-DNS and named virtual IP address networking. You might want to explore these network options in more detail.","title":"Discover deployed services"},{"location":"service-discovery/#discover-deployed-services","text":"","title":"Discover deployed services"},{"location":"service-discovery/#before-you-begin","text":"Before starting this tutorial, you should verify the following: You have access to a running DC/OS cluster with at least at least one master node and three agent nodes. You have access to a computer where the DC/OS CLI is installed. You have the sample dcos-101/app1 application deployed and running in your cluster. You have the domain information command-line utility dig available on the computer you use to connect to the cluster. The dig utility is part of the DNS BIND utilities that are installed by default with most Linux distributions.","title":"Before you begin"},{"location":"service-discovery/#learning-objectives","text":"By completing this tutorial, you will learn: The service discovery options available in DC/OS. How DC/OS resolves service addresses to find running instances.","title":"Learning objectives"},{"location":"service-discovery/#select-a-service-discovery-method","text":"In a DC/OS cluster, service discovery provides a method for finding applications regardless of where they might be running in the cluster. With service discovery, you can find where deployed services are running in the DC/OS cluster in one of two ways: By resolving private or public agent node IP addresses for tasks through the Mesos domain naming service (Mesos-DNS) . Through manually-set named virtual IP addresses that are not resolved through the Mesos domain naming service. Keeping service discovery separate from traditional DNS name resolution (in which service records are associated with a specific physical or virtual IP addresses) is particularly useful in cases where applications fail and might be restarted on a different host.","title":"Select a service discovery method"},{"location":"service-discovery/#using-mesos-dns","text":"The most common service discovery option is Mesos-DNS. Mesos-DNS provides a relatively simple method for finding applications inside the cluster. Mesos-DNS assigns DNS entries for every task. These task-specific DNS entries are then resolvable from any node in the cluster. The naming pattern for Mesos-DNS entries is task.scheduler.mesos Because the default scheduler for the Redis service you have deployed is Marathon , the Mesos-DNS name for your Redis service is redis.marathon.mesos or redis-tutorial.marathon.mesos. You can use DNS query tools, such as dig , to retrieve address (A) and service (SRV) record from the DNS nameserver, which in a DC/OS cluster is configured to point at Mesos-DNS. For example, you can run a command similar to this to find the IP address (A) record where an instance of the redis.marathon.mesos service is running: dig redis.marathon.mesos This command returns an answer section be similar to the following indicating that the service is running on the host with the IP address 10.0.0.43: ;; ANSWER SECTION: redis.marathon.mesos. 60 IN A 10.0.0.43 To connect to the service, you also need to know the port. In retrieve this information, Mesos-DNS also assigns each Marathon app a service (SRV) record containing the port number. You can run a command similar to the following to find the Redis SRV record: dig srv _redis._tcp.marathon.mesos This command returns an answer section be similar to the following indicating that the Redis service is running on 10.0.0.43:30585 ``` ;; ANSWER SECTION: _redis._tcp.marathon.mesos. 60 IN SRV 0 0 30585 redis-1y1hj-s1.marathon.mesos. ;; ADDITIONAL SECTION: redis-1y1hj-s1.marathon.mesos. 60 IN A 10.0.0.43 ``` Using Mesos-DNS for service discovery is appropriate for many applications, but has the following drawbacks: Applications sometimes cache DNS entries for efficiency and therefore might not provide updated address information in some cases, such as after a task failure. You need to use SRV DNS records to retrieve information about the allocated ports. While applications commonly understand DNS A records, not all applications support SRV records.","title":"Using Mesos-DNS"},{"location":"service-discovery/#using-named-virtual-ip-addresses","text":"Named virtual IP addresses (VIPs) enable you to assign name/port pairs to your apps. With this type of service discovery, you can give apps recognizable names with predictable port information. Virtual IP addresses also allow you to take advantage of DC/OS layer-4 load balancing when there are multiple instances of an application. For example, you can assign a named virtual IP address to the Redis service by adding the following to the app definition for the package: \"VIP_0\": \"/redis:6379\" The full name is then generated using the following format: vip-name.scheduler.l4lb.thisdcos.directory:vip-port . This is the discovery method used in the sample application for access the Redis service from within the cluster at redis.marathon.l4lb.thisdcos.directory:6379 . Named VIPs load balance the IP address/port pair using an intelligent algorithm to ensure optimal routing of the traffic in relation to the original requestor, and also provide a local caching layer for high performance. They also allow you to give your apps meaningful names and select a specific port. Because of these advantages over Mesos-DNS,in most cases you should use named virtual IP addresses for service discovery in DC/OS.","title":"Using named virtual IP addresses"},{"location":"service-discovery/#next-steps","text":"You know how to use service discovery to connect to your application from within your DC/OS cluster, and have learned about the two mechanisms for service discovery available in DC/OS.","title":"Next steps"},{"location":"service-discovery/#related-topics","text":"Now that you have a basic introduction to Mesos-DNS and named virtual IP address networking. You might want to explore these network options in more detail.","title":"Related topics"},{"location":"start-here/start-here/","text":"Create a cluster Let's start your DC/OS tour by creating a cluster This tutorial demonstrates the basic steps for creating a small DC/OS cluster using the most common default configuration options and verifying access to the cluster. You must successfully complete the steps in this tutorial before you can perform any other administrative tasks or explore additional features. After completing this tutorial, you will have a single DC/OS cluster consisting of: One master node . Two private agent nodes . One public agent node . The tutorial takes approximately 20 minutes to complete. If you need additional information about hardware or software system requirements or help performing any step, see the setup instructions . Before you begin To get started with this tutorial: - You must have access to a physical computer or virtual machine image with a supported operating system. You must have an account with administrative privileges for the local operating system or the cloud provider instance where you plan to install DC/OS. You must have a supported version of Docker installed. Before starting the tutorial, you should also verify that you have the following skills and information required to complete tutorial tasks. Knowledge Basic understanding of cluster-related concepts, software containers, distributed workload processing, and application deployment. General familiarity with Linux system administration and how to use common command-line programs for working with files and directories, such as ls , mkdir , and rm commands. You should also know how to display usage information and command-specific man pages. Skills Basic text-editing skills and experience working with configuration files, JSON-formatted files, and text editors such as vim or nano . Experience using a terminal shell and secure shell (SSH) connections to access remote servers and workstations. You must be able to start SSH sessions using a client application such as iTerm, Konsole, gnome-terminal, or PuTTY. Learning objectives For simplicity, this tutorial guides you through creating a cluster with a single master node. To run production workloads, however, you should have multiple master nodes. By completing this tutorial, you will learn: How to download the installation package and create a bootstrap node for distributing installation files. How to distribute the installation package and designate a computer as a master node. How to distribute the installation package and configure private and public agent nodes. How to open the DC/OS web-based administrative console and use it to view basic information about your cluster in a web browser. How to install the DC/OS command-line interface and use it to explore your cluster. Preview of what you'll do You need to perform the following key tasks to create a new DC/OS cluster: - Prepare a bootstrap node . Configure a DC/OS master node . Configure DC/OS private agent nodes . Configure a DC/OS public agent node . Prepare a bootstrap node Identify a computer to act as the bootstrap node for the new cluster. The bootstrap node computer provides a centralized location for configuring and distributing files for the DC/OS cluster. The bootstrap node: - Must be able to connect over the network to all cluster nodes using SSH. Can be backed up and shut down after installation is complete. Should not be included in the DC/OS cluster. Log on to the bootstrap node using administrative credentials. Check whether the Docker system process ( dockerd ) is available by running a command similar to the following: bash docker info This command returns an error if the Docker daemon process is not available. Download DC/OS Open Source or DC/OS Enterprise artifacts to the bootstrap node. Extract the contents from the file you downloaded by running a command similar to the following: bash /bin/sh dcos_generate_config.ee.sh Change to the DC/OS configuration directory and verify you have the config.yaml file: bash cd genconf ls -al Initially, the config.yaml file only contains a few lines that you can use as a skeleton for setting DC/OS configuration options. Prepare the cluster configuration files Open the config.yaml file in a text editor to customize the settings for this tutorial. For example, modify the file with settings similar to the following: bash bootstrap_url: http://10.0.0.100 cluster_name: 'Mesosphere DC/OS Tutorial' customer_key: 12345-12345-12345-12345-12345-123456 exhibitor_storage_backend: static master_discovery: static master_list: - 10.0.0.50 resolvers: - 169.254.169.253 - 127.0.0.1 security: permissive You can set many more basic and advanced configuration options using the config.yaml file. For information about the settings available and examples of the most commonly-used settings, see the advanced configuration reference and examples . Save your configuration settings. Add required scripts or files to the genconf directory. In addition to the config.yaml file, you should provide the following files in the genconf directory: - ip-detect - This script is required for all DC/OS clusters. - license.txt - This file is required for DC/OS Enterprise clusters. - fault-domain-detect - This script is required for DC/OS Enterprise clusters. Create the distribution center Run the DC/OS installation script to generate the customized build files for your cluster in the ./genconf/serve/ directory. bash sudo bash dcos_generate_config.ee.sh Prepare a web server NGINX Docker container to share the customized build files for distribution by running the following command on the bootstrap node: bash sudo docker run -d -p 80:80 -v $PWD/genconf/serve:/usr/share/nginx/html:ro nginx Create the master node Open a terminal shell on the bootstrap node, then start a secure shell (SSH) session to connect to the master node. bash ssh master-ip Create a new directory for the DC/OS master node files and navigate to it. bash mkdir /tmp/dcos cd /tmp/dcos Download the DC/OS installation script from the NGINX Docker container, replacing bootstrap-ip and port with the settings you specified for the bootstrap_url in the config.yaml file: bash curl -O http:// bootstrap-ip : your_port /dcos_install.sh Run the following command to install DC/OS on the master node. bash sudo bash dcos_install.sh master In a production environment, you would repeat these steps to create two or four additional master nodes. Configure private agent nodes Open a terminal shell on the bootstrap node, then start a secure shell (SSH) session to connect to the first private agent node. bash ssh agent-ip Create a new directory for the DC/OS agent files and navigate to it. bash mkdir /tmp/dcos cd /tmp/dcos Download the DC/OS installation script from the NGINX Docker container, replacing bootstrap-ip and port with the settings you specified for the bootstrap_url in the config.yaml file: bash curl -O http:// bootstrap-ip : your_port /dcos_install.sh Run the following command to install DC/OS and designate this node as a private agent node. bash sudo bash dcos_install.sh slave 1. Repeat these steps to create a second private agent node. In a production environment, you would automate these steps to create as many private agent nodes as you need. Configure the public agent node Open a terminal shell on the bootstrap node, then start a secure shell (SSH) session to connect to the public agent node. bash ssh agent-ip Create a new directory for the DC/OS agent files and navigate to it. bash mkdir /tmp/dcos cd /tmp/dcos Download the DC/OS installation script from the NGINX Docker container, replacing bootstrap-ip and port with the settings you specified for the bootstrap_url in the config.yaml file: bash curl -O http:// bootstrap-ip : your_port /dcos_install.sh Run the following command to install DC/OS and designate this node as a public agent node. bash sudo bash dcos_install.sh slave_public Verify your cluster is ready to use Open a web browser and navigate to the master node IP address to access the DC/OS web-based administrative console. For example, if the master node IP address is 192.168.47.1, enter http://192.168.47.1 as the URL in the browser address bar. Type your administrative user name and password, then click Log in . If the connection is successful, the DC/OS dashboard is displayed. Congratulations! You have successfully created your first DC/OS cluster. You can now start exploring what you can do using this cluster in subsequent tutorials. Next steps Now that you have a small cluster running, you can install the DC/OS command-line interface (CLI) and start exploring administrative and operational tasks. Install the command-line interface Install your first service from the package repository Deploy your first sample application Related topics This tutorial focused on preparing and installing the DC/OS cluster interactively using a simple configuration file and a few manually entered commands. More about your installation options There are several other methods you can use to install the DC/OS cluster. For example, there are other installation options if you are installing DC/OS on a public cloud from a public cloud provider such as AWS, Azure, or the Google Cloud Platform. For information about other installation options, see the following topics: DC/OS on AWS using the Universal Installer DC/OS on Azure using the Universal Installer DC/OS on GCP using the Universal Installer Other Installation methods More about cluster architecture and components For an overview of the DC/OS platform and the components that make up the architectural layers of the platform, see the Architectural overview . If you want to know more about the DC/OS architecture and key components, see the following topics: Mesos containers and orchestration. Marathon framework and application definitions. Metronome job management and scheduling.","title":"Create a cluster"},{"location":"start-here/start-here/#create-a-cluster","text":"","title":"Create a cluster"},{"location":"start-here/start-here/#before-you-begin","text":"To get started with this tutorial: - You must have access to a physical computer or virtual machine image with a supported operating system. You must have an account with administrative privileges for the local operating system or the cloud provider instance where you plan to install DC/OS. You must have a supported version of Docker installed. Before starting the tutorial, you should also verify that you have the following skills and information required to complete tutorial tasks.","title":"Before you begin"},{"location":"start-here/start-here/#knowledge","text":"Basic understanding of cluster-related concepts, software containers, distributed workload processing, and application deployment. General familiarity with Linux system administration and how to use common command-line programs for working with files and directories, such as ls , mkdir , and rm commands. You should also know how to display usage information and command-specific man pages.","title":"Knowledge"},{"location":"start-here/start-here/#skills","text":"Basic text-editing skills and experience working with configuration files, JSON-formatted files, and text editors such as vim or nano . Experience using a terminal shell and secure shell (SSH) connections to access remote servers and workstations. You must be able to start SSH sessions using a client application such as iTerm, Konsole, gnome-terminal, or PuTTY.","title":"Skills"},{"location":"start-here/start-here/#learning-objectives","text":"For simplicity, this tutorial guides you through creating a cluster with a single master node. To run production workloads, however, you should have multiple master nodes. By completing this tutorial, you will learn: How to download the installation package and create a bootstrap node for distributing installation files. How to distribute the installation package and designate a computer as a master node. How to distribute the installation package and configure private and public agent nodes. How to open the DC/OS web-based administrative console and use it to view basic information about your cluster in a web browser. How to install the DC/OS command-line interface and use it to explore your cluster.","title":"Learning objectives"},{"location":"start-here/start-here/#preview-of-what-youll-do","text":"You need to perform the following key tasks to create a new DC/OS cluster: - Prepare a bootstrap node . Configure a DC/OS master node . Configure DC/OS private agent nodes . Configure a DC/OS public agent node .","title":"Preview of what you'll do"},{"location":"start-here/start-here/#prepare-a-bootstrap-node","text":"Identify a computer to act as the bootstrap node for the new cluster. The bootstrap node computer provides a centralized location for configuring and distributing files for the DC/OS cluster. The bootstrap node: - Must be able to connect over the network to all cluster nodes using SSH. Can be backed up and shut down after installation is complete. Should not be included in the DC/OS cluster. Log on to the bootstrap node using administrative credentials. Check whether the Docker system process ( dockerd ) is available by running a command similar to the following: bash docker info This command returns an error if the Docker daemon process is not available. Download DC/OS Open Source or DC/OS Enterprise artifacts to the bootstrap node. Extract the contents from the file you downloaded by running a command similar to the following: bash /bin/sh dcos_generate_config.ee.sh Change to the DC/OS configuration directory and verify you have the config.yaml file: bash cd genconf ls -al Initially, the config.yaml file only contains a few lines that you can use as a skeleton for setting DC/OS configuration options.","title":"Prepare a bootstrap node"},{"location":"start-here/start-here/#prepare-the-cluster-configuration-files","text":"Open the config.yaml file in a text editor to customize the settings for this tutorial. For example, modify the file with settings similar to the following: bash bootstrap_url: http://10.0.0.100 cluster_name: 'Mesosphere DC/OS Tutorial' customer_key: 12345-12345-12345-12345-12345-123456 exhibitor_storage_backend: static master_discovery: static master_list: - 10.0.0.50 resolvers: - 169.254.169.253 - 127.0.0.1 security: permissive You can set many more basic and advanced configuration options using the config.yaml file. For information about the settings available and examples of the most commonly-used settings, see the advanced configuration reference and examples . Save your configuration settings. Add required scripts or files to the genconf directory. In addition to the config.yaml file, you should provide the following files in the genconf directory: - ip-detect - This script is required for all DC/OS clusters. - license.txt - This file is required for DC/OS Enterprise clusters. - fault-domain-detect - This script is required for DC/OS Enterprise clusters.","title":"Prepare the cluster configuration files"},{"location":"start-here/start-here/#create-the-distribution-center","text":"Run the DC/OS installation script to generate the customized build files for your cluster in the ./genconf/serve/ directory. bash sudo bash dcos_generate_config.ee.sh Prepare a web server NGINX Docker container to share the customized build files for distribution by running the following command on the bootstrap node: bash sudo docker run -d -p 80:80 -v $PWD/genconf/serve:/usr/share/nginx/html:ro nginx","title":"Create the distribution center"},{"location":"start-here/start-here/#create-the-master-node","text":"Open a terminal shell on the bootstrap node, then start a secure shell (SSH) session to connect to the master node. bash ssh master-ip Create a new directory for the DC/OS master node files and navigate to it. bash mkdir /tmp/dcos cd /tmp/dcos Download the DC/OS installation script from the NGINX Docker container, replacing bootstrap-ip and port with the settings you specified for the bootstrap_url in the config.yaml file: bash curl -O http:// bootstrap-ip : your_port /dcos_install.sh Run the following command to install DC/OS on the master node. bash sudo bash dcos_install.sh master In a production environment, you would repeat these steps to create two or four additional master nodes.","title":"Create the master node"},{"location":"start-here/start-here/#configure-private-agent-nodes","text":"Open a terminal shell on the bootstrap node, then start a secure shell (SSH) session to connect to the first private agent node. bash ssh agent-ip Create a new directory for the DC/OS agent files and navigate to it. bash mkdir /tmp/dcos cd /tmp/dcos Download the DC/OS installation script from the NGINX Docker container, replacing bootstrap-ip and port with the settings you specified for the bootstrap_url in the config.yaml file: bash curl -O http:// bootstrap-ip : your_port /dcos_install.sh Run the following command to install DC/OS and designate this node as a private agent node. bash sudo bash dcos_install.sh slave 1. Repeat these steps to create a second private agent node. In a production environment, you would automate these steps to create as many private agent nodes as you need.","title":"Configure private agent nodes"},{"location":"start-here/start-here/#configure-the-public-agent-node","text":"Open a terminal shell on the bootstrap node, then start a secure shell (SSH) session to connect to the public agent node. bash ssh agent-ip Create a new directory for the DC/OS agent files and navigate to it. bash mkdir /tmp/dcos cd /tmp/dcos Download the DC/OS installation script from the NGINX Docker container, replacing bootstrap-ip and port with the settings you specified for the bootstrap_url in the config.yaml file: bash curl -O http:// bootstrap-ip : your_port /dcos_install.sh Run the following command to install DC/OS and designate this node as a public agent node. bash sudo bash dcos_install.sh slave_public","title":"Configure the public agent node"},{"location":"start-here/start-here/#verify-your-cluster-is-ready-to-use","text":"Open a web browser and navigate to the master node IP address to access the DC/OS web-based administrative console. For example, if the master node IP address is 192.168.47.1, enter http://192.168.47.1 as the URL in the browser address bar. Type your administrative user name and password, then click Log in . If the connection is successful, the DC/OS dashboard is displayed. Congratulations! You have successfully created your first DC/OS cluster. You can now start exploring what you can do using this cluster in subsequent tutorials.","title":"Verify your cluster is ready to use"},{"location":"start-here/start-here/#next-steps","text":"Now that you have a small cluster running, you can install the DC/OS command-line interface (CLI) and start exploring administrative and operational tasks. Install the command-line interface Install your first service from the package repository Deploy your first sample application","title":"Next steps"},{"location":"start-here/start-here/#related-topics","text":"This tutorial focused on preparing and installing the DC/OS cluster interactively using a simple configuration file and a few manually entered commands.","title":"Related topics"},{"location":"start-here/start-here/#more-about-your-installation-options","text":"There are several other methods you can use to install the DC/OS cluster. For example, there are other installation options if you are installing DC/OS on a public cloud from a public cloud provider such as AWS, Azure, or the Google Cloud Platform. For information about other installation options, see the following topics: DC/OS on AWS using the Universal Installer DC/OS on Azure using the Universal Installer DC/OS on GCP using the Universal Installer Other Installation methods","title":"More about your installation options"},{"location":"start-here/start-here/#more-about-cluster-architecture-and-components","text":"For an overview of the DC/OS platform and the components that make up the architectural layers of the platform, see the Architectural overview . If you want to know more about the DC/OS architecture and key components, see the following topics: Mesos containers and orchestration. Marathon framework and application definitions. Metronome job management and scheduling.","title":"More about cluster architecture and components"}]}